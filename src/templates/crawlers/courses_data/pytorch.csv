Title,Link,Content
Quickstart,https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html,"<div class=""section"" id=""quickstart"">
 <h1>
  Quickstart
  <a class=""headerlink"" href=""#quickstart"" title=""Permalink to this heading"">
   ¶
  </a>
 </h1>
 <p class=""date-info-last-verified"" style=""color: #6c6c6d; font-size: small;"">
  Created On: Feb 09, 2021 | Last Updated: Jan 24, 2025 | Last Verified: Not Verified
 </p>
 <p>
  This section runs through the API for common tasks in machine learning. Refer to the links in each section to dive deeper.
 </p>
 <div class=""section"" id=""working-with-data"">
  <h2>
   Working with data
   <a class=""headerlink"" href=""#working-with-data"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   PyTorch has two
   <a class=""reference external"" href=""https://pytorch.org/docs/stable/data.html"">
    primitives to work with data
   </a>
   :
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     torch.utils.data.DataLoader
    </span>
   </code>
   and
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     torch.utils.data.Dataset
    </span>
   </code>
   .
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     Dataset
    </span>
   </code>
   stores the samples and their corresponding labels, and
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     DataLoader
    </span>
   </code>
   wraps an iterable around
the
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     Dataset
    </span>
   </code>
   .
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""kn"">import</span><span class=""w""> </span><span class=""nn"">torch</span>
<span class=""kn"">from</span><span class=""w""> </span><span class=""nn"">torch</span><span class=""w""> </span><span class=""kn"">import</span> <span class=""n"">nn</span>
<span class=""kn"">from</span><span class=""w""> </span><span class=""nn"">torch.utils.data</span><span class=""w""> </span><span class=""kn"">import</span> <a class=""sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" title=""torch.utils.data.DataLoader""><span class=""n"">DataLoader</span></a>
<span class=""kn"">from</span><span class=""w""> </span><span class=""nn"">torchvision</span><span class=""w""> </span><span class=""kn"">import</span> <span class=""n"">datasets</span>
<span class=""kn"">from</span><span class=""w""> </span><span class=""nn"">torchvision.transforms</span><span class=""w""> </span><span class=""kn"">import</span> <a class=""sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class"" href=""https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor"" title=""torchvision.transforms.ToTensor""><span class=""n"">ToTensor</span></a>
</pre>
   </div>
  </div>
  <p>
   PyTorch offers domain-specific libraries such as
   <a class=""reference external"" href=""https://pytorch.org/text/stable/index.html"">
    TorchText
   </a>
   ,
   <a class=""reference external"" href=""https://pytorch.org/vision/stable/index.html"">
    TorchVision
   </a>
   , and
   <a class=""reference external"" href=""https://pytorch.org/audio/stable/index.html"">
    TorchAudio
   </a>
   ,
all of which include datasets. For this tutorial, we  will be using a TorchVision dataset.
  </p>
  <p>
   The
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     torchvision.datasets
    </span>
   </code>
   module contains
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     Dataset
    </span>
   </code>
   objects for many real-world vision data like
CIFAR, COCO (
   <a class=""reference external"" href=""https://pytorch.org/vision/stable/datasets.html"">
    full list here
   </a>
   ). In this tutorial, we
use the FashionMNIST dataset. Every TorchVision
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     Dataset
    </span>
   </code>
   includes two arguments:
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     transform
    </span>
   </code>
   and
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     target_transform
    </span>
   </code>
   to modify the samples and labels respectively.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""c1""># Download training data from open datasets.</span>
<a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">training_data</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">datasets</span><span class=""o"">.</span><span class=""n"">FashionMNIST</span></a><span class=""p"">(</span>
    <span class=""n"">root</span><span class=""o"">=</span><span class=""s2"">""data""</span><span class=""p"">,</span>
    <span class=""n"">train</span><span class=""o"">=</span><span class=""kc"">True</span><span class=""p"">,</span>
    <span class=""n"">download</span><span class=""o"">=</span><span class=""kc"">True</span><span class=""p"">,</span>
    <span class=""n"">transform</span><span class=""o"">=</span><a class=""sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class"" href=""https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor"" title=""torchvision.transforms.ToTensor""><span class=""n"">ToTensor</span></a><span class=""p"">(),</span>
<span class=""p"">)</span>

<span class=""c1""># Download test data from open datasets.</span>
<a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">test_data</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">datasets</span><span class=""o"">.</span><span class=""n"">FashionMNIST</span></a><span class=""p"">(</span>
    <span class=""n"">root</span><span class=""o"">=</span><span class=""s2"">""data""</span><span class=""p"">,</span>
    <span class=""n"">train</span><span class=""o"">=</span><span class=""kc"">False</span><span class=""p"">,</span>
    <span class=""n"">download</span><span class=""o"">=</span><span class=""kc"">True</span><span class=""p"">,</span>
    <span class=""n"">transform</span><span class=""o"">=</span><a class=""sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class"" href=""https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor"" title=""torchvision.transforms.ToTensor""><span class=""n"">ToTensor</span></a><span class=""p"">(),</span>
<span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>  0%|          | 0.00/26.4M [00:00&lt;?, ?B/s]
  0%|          | 65.5k/26.4M [00:00&lt;01:12, 362kB/s]
  1%|          | 164k/26.4M [00:00&lt;00:42, 620kB/s]
  1%|1         | 393k/26.4M [00:00&lt;00:25, 1.00MB/s]
  5%|5         | 1.38M/26.4M [00:00&lt;00:06, 3.60MB/s]
 12%|#1        | 3.08M/26.4M [00:00&lt;00:03, 6.33MB/s]
 30%|###       | 7.96M/26.4M [00:00&lt;00:01, 17.4MB/s]
 43%|####3     | 11.4M/26.4M [00:00&lt;00:00, 21.9MB/s]
 57%|#####7    | 15.1M/26.4M [00:01&lt;00:00, 22.2MB/s]
 74%|#######4  | 19.6M/26.4M [00:01&lt;00:00, 27.9MB/s]
 89%|########9 | 23.6M/26.4M [00:01&lt;00:00, 31.0MB/s]
100%|##########| 26.4M/26.4M [00:01&lt;00:00, 19.1MB/s]

  0%|          | 0.00/29.5k [00:00&lt;?, ?B/s]
100%|##########| 29.5k/29.5k [00:00&lt;00:00, 327kB/s]

  0%|          | 0.00/4.42M [00:00&lt;?, ?B/s]
  1%|1         | 65.5k/4.42M [00:00&lt;00:12, 362kB/s]
  5%|5         | 229k/4.42M [00:00&lt;00:06, 681kB/s]
 21%|##        | 918k/4.42M [00:00&lt;00:01, 2.49MB/s]
 44%|####3     | 1.93M/4.42M [00:00&lt;00:00, 4.13MB/s]
100%|##########| 4.42M/4.42M [00:00&lt;00:00, 6.08MB/s]

  0%|          | 0.00/5.15k [00:00&lt;?, ?B/s]
100%|##########| 5.15k/5.15k [00:00&lt;00:00, 48.3MB/s]
</pre>
   </div>
  </div>
  <p>
   We pass the
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     Dataset
    </span>
   </code>
   as an argument to
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     DataLoader
    </span>
   </code>
   . This wraps an iterable over our dataset, and supports
automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element
in the dataloader iterable will return a batch of 64 features and labels.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""n"">batch_size</span> <span class=""o"">=</span> <span class=""mi"">64</span>

<span class=""c1""># Create data loaders.</span>
<a class=""sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" title=""torch.utils.data.DataLoader""><span class=""n"">train_dataloader</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" title=""torch.utils.data.DataLoader""><span class=""n"">DataLoader</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">training_data</span></a><span class=""p"">,</span> <span class=""n"">batch_size</span><span class=""o"">=</span><span class=""n"">batch_size</span><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" title=""torch.utils.data.DataLoader""><span class=""n"">test_dataloader</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" title=""torch.utils.data.DataLoader""><span class=""n"">DataLoader</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">test_data</span></a><span class=""p"">,</span> <span class=""n"">batch_size</span><span class=""o"">=</span><span class=""n"">batch_size</span><span class=""p"">)</span>

<span class=""k"">for</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">X</span></a><span class=""p"">,</span> <span class=""n"">y</span> <span class=""ow"">in</span> <a class=""sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" title=""torch.utils.data.DataLoader""><span class=""n"">test_dataloader</span></a><span class=""p"">:</span>
    <span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Shape of X [N, C, H, W]: </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/size.html#torch.Size"" title=""torch.Size""><span class=""n"">X</span><span class=""o"">.</span><span class=""n"">shape</span></a><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
    <span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Shape of y: </span><span class=""si"">{</span><span class=""n"">y</span><span class=""o"">.</span><span class=""n"">shape</span><span class=""si"">}</span><span class=""s2""> </span><span class=""si"">{</span><span class=""n"">y</span><span class=""o"">.</span><span class=""n"">dtype</span><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
    <span class=""k"">break</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])
Shape of y: torch.Size([64]) torch.int64
</pre>
   </div>
  </div>
  <p>
   Read more about
   <a class=""reference external"" href=""data_tutorial.html"">
    loading data in PyTorch
   </a>
   .
  </p>
  <hr class=""docutils""/>
 </div>
 <div class=""section"" id=""creating-models"">
  <h2>
   Creating Models
   <a class=""headerlink"" href=""#creating-models"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   To define a neural network in PyTorch, we create a class that inherits
from
   <a class=""reference external"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html"">
    nn.Module
   </a>
   . We define the layers of the network
in the
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     __init__
    </span>
   </code>
   function and specify how data will pass through the network in the
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     forward
    </span>
   </code>
   function. To accelerate
operations in the neural network, we move it to the
   <a class=""reference external"" href=""https://pytorch.org/docs/stable/torch.html#accelerators"">
    accelerator
   </a>
   such as CUDA, MPS, MTIA, or XPU. If the current accelerator is available, we will use it. Otherwise, we use the CPU.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""n"">device</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-accelerator sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.accelerator.current_accelerator.html#torch.accelerator.current_accelerator"" title=""torch.accelerator.current_accelerator""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">accelerator</span><span class=""o"">.</span><span class=""n"">current_accelerator</span></a><span class=""p"">()</span><span class=""o"">.</span><span class=""n"">type</span> <span class=""k"">if</span> <a class=""sphx-glr-backref-module-torch-accelerator sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.accelerator.is_available.html#torch.accelerator.is_available"" title=""torch.accelerator.is_available""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">accelerator</span><span class=""o"">.</span><span class=""n"">is_available</span></a><span class=""p"">()</span> <span class=""k"">else</span> <span class=""s2"">""cpu""</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Using </span><span class=""si"">{</span><span class=""n"">device</span><span class=""si"">}</span><span class=""s2""> device""</span><span class=""p"">)</span>

<span class=""c1""># Define model</span>
<span class=""k"">class</span><span class=""w""> </span><span class=""nc"">NeuralNetwork</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module"" title=""torch.nn.Module""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Module</span></a><span class=""p"">):</span>
    <span class=""k"">def</span><span class=""w""> </span><span class=""fm"">__init__</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">):</span>
        <span class=""nb"">super</span><span class=""p"">()</span><span class=""o"">.</span><span class=""fm"">__init__</span><span class=""p"">()</span>
        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">flatten</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten"" title=""torch.nn.Flatten""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Flatten</span></a><span class=""p"">()</span>
        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">linear_relu_stack</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential"" title=""torch.nn.Sequential""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Sequential</span></a><span class=""p"">(</span>
            <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear"" title=""torch.nn.Linear""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Linear</span></a><span class=""p"">(</span><span class=""mi"">28</span><span class=""o"">*</span><span class=""mi"">28</span><span class=""p"">,</span> <span class=""mi"">512</span><span class=""p"">),</span>
            <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU"" title=""torch.nn.ReLU""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">ReLU</span></a><span class=""p"">(),</span>
            <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear"" title=""torch.nn.Linear""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Linear</span></a><span class=""p"">(</span><span class=""mi"">512</span><span class=""p"">,</span> <span class=""mi"">512</span><span class=""p"">),</span>
            <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU"" title=""torch.nn.ReLU""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">ReLU</span></a><span class=""p"">(),</span>
            <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear"" title=""torch.nn.Linear""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Linear</span></a><span class=""p"">(</span><span class=""mi"">512</span><span class=""p"">,</span> <span class=""mi"">10</span><span class=""p"">)</span>
        <span class=""p"">)</span>

    <span class=""k"">def</span><span class=""w""> </span><span class=""nf"">forward</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">x</span></a><span class=""p"">):</span>
        <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">x</span></a> <span class=""o"">=</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">flatten</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">x</span></a><span class=""p"">)</span>
        <span class=""n"">logits</span> <span class=""o"">=</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">linear_relu_stack</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">x</span></a><span class=""p"">)</span>
        <span class=""k"">return</span> <span class=""n"">logits</span>

<span class=""n"">model</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module"" title=""torch.nn.Module""><span class=""n"">NeuralNetwork</span></a><span class=""p"">()</span><span class=""o"">.</span><span class=""n"">to</span><span class=""p"">(</span><span class=""n"">device</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""n"">model</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>Using cuda device
NeuralNetwork(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=784, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=10, bias=True)
  )
)
</pre>
   </div>
  </div>
  <p>
   Read more about
   <a class=""reference external"" href=""buildmodel_tutorial.html"">
    building neural networks in PyTorch
   </a>
   .
  </p>
  <hr class=""docutils""/>
 </div>
 <div class=""section"" id=""optimizing-the-model-parameters"">
  <h2>
   Optimizing the Model Parameters
   <a class=""headerlink"" href=""#optimizing-the-model-parameters"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   To train a model, we need a
   <a class=""reference external"" href=""https://pytorch.org/docs/stable/nn.html#loss-functions"">
    loss function
   </a>
   and an
   <a class=""reference external"" href=""https://pytorch.org/docs/stable/optim.html"">
    optimizer
   </a>
   .
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"" title=""torch.nn.CrossEntropyLoss""><span class=""n"">loss_fn</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"" title=""torch.nn.CrossEntropyLoss""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">CrossEntropyLoss</span></a><span class=""p"">()</span>
<a class=""sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD"" title=""torch.optim.SGD""><span class=""n"">optimizer</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD"" title=""torch.optim.SGD""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">optim</span><span class=""o"">.</span><span class=""n"">SGD</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters"" title=""torch.nn.Module.parameters""><span class=""n"">model</span><span class=""o"">.</span><span class=""n"">parameters</span></a><span class=""p"">(),</span> <span class=""n"">lr</span><span class=""o"">=</span><span class=""mf"">1e-3</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <p>
   In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and
backpropagates the prediction error to adjust the model’s parameters.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""k"">def</span><span class=""w""> </span><span class=""nf"">train</span><span class=""p"">(</span><span class=""n"">dataloader</span><span class=""p"">,</span> <span class=""n"">model</span><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"" title=""torch.nn.CrossEntropyLoss""><span class=""n"">loss_fn</span></a><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD"" title=""torch.optim.SGD""><span class=""n"">optimizer</span></a><span class=""p"">):</span>
    <span class=""n"">size</span> <span class=""o"">=</span> <span class=""nb"">len</span><span class=""p"">(</span><span class=""n"">dataloader</span><span class=""o"">.</span><span class=""n"">dataset</span><span class=""p"">)</span>
    <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train"" title=""torch.nn.Module.train""><span class=""n"">model</span><span class=""o"">.</span><span class=""n"">train</span></a><span class=""p"">()</span>
    <span class=""k"">for</span> <span class=""n"">batch</span><span class=""p"">,</span> <span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">X</span></a><span class=""p"">,</span> <span class=""n"">y</span><span class=""p"">)</span> <span class=""ow"">in</span> <span class=""nb"">enumerate</span><span class=""p"">(</span><span class=""n"">dataloader</span><span class=""p"">):</span>
        <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">X</span></a><span class=""p"">,</span> <span class=""n"">y</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">X</span></a><span class=""o"">.</span><span class=""n"">to</span><span class=""p"">(</span><span class=""n"">device</span><span class=""p"">),</span> <span class=""n"">y</span><span class=""o"">.</span><span class=""n"">to</span><span class=""p"">(</span><span class=""n"">device</span><span class=""p"">)</span>

        <span class=""c1""># Compute prediction error</span>
        <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">pred</span></a> <span class=""o"">=</span> <span class=""n"">model</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">X</span></a><span class=""p"">)</span>
        <span class=""n"">loss</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"" title=""torch.nn.CrossEntropyLoss""><span class=""n"">loss_fn</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">pred</span></a><span class=""p"">,</span> <span class=""n"">y</span><span class=""p"">)</span>

        <span class=""c1""># Backpropagation</span>
        <span class=""n"">loss</span><span class=""o"">.</span><span class=""n"">backward</span><span class=""p"">()</span>
        <a class=""sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-method"" href=""https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.step"" title=""torch.optim.SGD.step""><span class=""n"">optimizer</span><span class=""o"">.</span><span class=""n"">step</span></a><span class=""p"">()</span>
        <a class=""sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-method"" href=""https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.zero_grad"" title=""torch.optim.SGD.zero_grad""><span class=""n"">optimizer</span><span class=""o"">.</span><span class=""n"">zero_grad</span></a><span class=""p"">()</span>

        <span class=""k"">if</span> <span class=""n"">batch</span> <span class=""o"">%</span> <span class=""mi"">100</span> <span class=""o"">==</span> <span class=""mi"">0</span><span class=""p"">:</span>
            <span class=""n"">loss</span><span class=""p"">,</span> <span class=""n"">current</span> <span class=""o"">=</span> <span class=""n"">loss</span><span class=""o"">.</span><span class=""n"">item</span><span class=""p"">(),</span> <span class=""p"">(</span><span class=""n"">batch</span> <span class=""o"">+</span> <span class=""mi"">1</span><span class=""p"">)</span> <span class=""o"">*</span> <span class=""nb"">len</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">X</span></a><span class=""p"">)</span>
            <span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""loss: </span><span class=""si"">{</span><span class=""n"">loss</span><span class=""si"">:</span><span class=""s2"">&gt;7f</span><span class=""si"">}</span><span class=""s2"">  [</span><span class=""si"">{</span><span class=""n"">current</span><span class=""si"">:</span><span class=""s2"">&gt;5d</span><span class=""si"">}</span><span class=""s2"">/</span><span class=""si"">{</span><span class=""n"">size</span><span class=""si"">:</span><span class=""s2"">&gt;5d</span><span class=""si"">}</span><span class=""s2"">]""</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <p>
   We also check the model’s performance against the test dataset to ensure it is learning.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""k"">def</span><span class=""w""> </span><span class=""nf"">test</span><span class=""p"">(</span><span class=""n"">dataloader</span><span class=""p"">,</span> <span class=""n"">model</span><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"" title=""torch.nn.CrossEntropyLoss""><span class=""n"">loss_fn</span></a><span class=""p"">):</span>
    <span class=""n"">size</span> <span class=""o"">=</span> <span class=""nb"">len</span><span class=""p"">(</span><span class=""n"">dataloader</span><span class=""o"">.</span><span class=""n"">dataset</span><span class=""p"">)</span>
    <span class=""n"">num_batches</span> <span class=""o"">=</span> <span class=""nb"">len</span><span class=""p"">(</span><span class=""n"">dataloader</span><span class=""p"">)</span>
    <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval"" title=""torch.nn.Module.eval""><span class=""n"">model</span><span class=""o"">.</span><span class=""n"">eval</span></a><span class=""p"">()</span>
    <span class=""n"">test_loss</span><span class=""p"">,</span> <span class=""n"">correct</span> <span class=""o"">=</span> <span class=""mi"">0</span><span class=""p"">,</span> <span class=""mi"">0</span>
    <span class=""k"">with</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad"" title=""torch.no_grad""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">no_grad</span></a><span class=""p"">():</span>
        <span class=""k"">for</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">X</span></a><span class=""p"">,</span> <span class=""n"">y</span> <span class=""ow"">in</span> <span class=""n"">dataloader</span><span class=""p"">:</span>
            <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">X</span></a><span class=""p"">,</span> <span class=""n"">y</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">X</span></a><span class=""o"">.</span><span class=""n"">to</span><span class=""p"">(</span><span class=""n"">device</span><span class=""p"">),</span> <span class=""n"">y</span><span class=""o"">.</span><span class=""n"">to</span><span class=""p"">(</span><span class=""n"">device</span><span class=""p"">)</span>
            <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">pred</span></a> <span class=""o"">=</span> <span class=""n"">model</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">X</span></a><span class=""p"">)</span>
            <span class=""n"">test_loss</span> <span class=""o"">+=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"" title=""torch.nn.CrossEntropyLoss""><span class=""n"">loss_fn</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">pred</span></a><span class=""p"">,</span> <span class=""n"">y</span><span class=""p"">)</span><span class=""o"">.</span><span class=""n"">item</span><span class=""p"">()</span>
            <span class=""n"">correct</span> <span class=""o"">+=</span> <span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">pred</span></a><span class=""o"">.</span><span class=""n"">argmax</span><span class=""p"">(</span><span class=""mi"">1</span><span class=""p"">)</span> <span class=""o"">==</span> <span class=""n"">y</span><span class=""p"">)</span><span class=""o"">.</span><span class=""n"">type</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype"" title=""torch.dtype""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">float</span></a><span class=""p"">)</span><span class=""o"">.</span><span class=""n"">sum</span><span class=""p"">()</span><span class=""o"">.</span><span class=""n"">item</span><span class=""p"">()</span>
    <span class=""n"">test_loss</span> <span class=""o"">/=</span> <span class=""n"">num_batches</span>
    <span class=""n"">correct</span> <span class=""o"">/=</span> <span class=""n"">size</span>
    <span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Test Error: </span><span class=""se"">\n</span><span class=""s2""> Accuracy: </span><span class=""si"">{</span><span class=""p"">(</span><span class=""mi"">100</span><span class=""o"">*</span><span class=""n"">correct</span><span class=""p"">)</span><span class=""si"">:</span><span class=""s2"">&gt;0.1f</span><span class=""si"">}</span><span class=""s2"">%, Avg loss: </span><span class=""si"">{</span><span class=""n"">test_loss</span><span class=""si"">:</span><span class=""s2"">&gt;8f</span><span class=""si"">}</span><span class=""s2""> </span><span class=""se"">\n</span><span class=""s2"">""</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <p>
   The training process is conducted over several iterations (
   <em>
    epochs
   </em>
   ). During each epoch, the model learns
parameters to make better predictions. We print the model’s accuracy and loss at each epoch; we’d like to see the
accuracy increase and the loss decrease with every epoch.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""n"">epochs</span> <span class=""o"">=</span> <span class=""mi"">5</span>
<span class=""k"">for</span> <span class=""n"">t</span> <span class=""ow"">in</span> <span class=""nb"">range</span><span class=""p"">(</span><span class=""n"">epochs</span><span class=""p"">):</span>
    <span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Epoch </span><span class=""si"">{</span><span class=""n"">t</span><span class=""o"">+</span><span class=""mi"">1</span><span class=""si"">}</span><span class=""se"">\n</span><span class=""s2"">-------------------------------""</span><span class=""p"">)</span>
    <span class=""n"">train</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" title=""torch.utils.data.DataLoader""><span class=""n"">train_dataloader</span></a><span class=""p"">,</span> <span class=""n"">model</span><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"" title=""torch.nn.CrossEntropyLoss""><span class=""n"">loss_fn</span></a><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD"" title=""torch.optim.SGD""><span class=""n"">optimizer</span></a><span class=""p"">)</span>
    <span class=""n"">test</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" title=""torch.utils.data.DataLoader""><span class=""n"">test_dataloader</span></a><span class=""p"">,</span> <span class=""n"">model</span><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"" title=""torch.nn.CrossEntropyLoss""><span class=""n"">loss_fn</span></a><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""s2"">""Done!""</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>Epoch 1
-------------------------------
loss: 2.303494  [   64/60000]
loss: 2.294637  [ 6464/60000]
loss: 2.277102  [12864/60000]
loss: 2.269977  [19264/60000]
loss: 2.254235  [25664/60000]
loss: 2.237146  [32064/60000]
loss: 2.231055  [38464/60000]
loss: 2.205037  [44864/60000]
loss: 2.203240  [51264/60000]
loss: 2.170889  [57664/60000]
Test Error:
 Accuracy: 53.9%, Avg loss: 2.168588

Epoch 2
-------------------------------
loss: 2.177787  [   64/60000]
loss: 2.168083  [ 6464/60000]
loss: 2.114910  [12864/60000]
loss: 2.130412  [19264/60000]
loss: 2.087473  [25664/60000]
loss: 2.039670  [32064/60000]
loss: 2.054274  [38464/60000]
loss: 1.985457  [44864/60000]
loss: 1.996023  [51264/60000]
loss: 1.917241  [57664/60000]
Test Error:
 Accuracy: 60.2%, Avg loss: 1.920374

Epoch 3
-------------------------------
loss: 1.951705  [   64/60000]
loss: 1.919516  [ 6464/60000]
loss: 1.808730  [12864/60000]
loss: 1.846550  [19264/60000]
loss: 1.740618  [25664/60000]
loss: 1.698733  [32064/60000]
loss: 1.708889  [38464/60000]
loss: 1.614436  [44864/60000]
loss: 1.646475  [51264/60000]
loss: 1.524308  [57664/60000]
Test Error:
 Accuracy: 61.4%, Avg loss: 1.547092

Epoch 4
-------------------------------
loss: 1.612695  [   64/60000]
loss: 1.570870  [ 6464/60000]
loss: 1.424730  [12864/60000]
loss: 1.489542  [19264/60000]
loss: 1.367256  [25664/60000]
loss: 1.373464  [32064/60000]
loss: 1.376744  [38464/60000]
loss: 1.304962  [44864/60000]
loss: 1.347154  [51264/60000]
loss: 1.230661  [57664/60000]
Test Error:
 Accuracy: 62.7%, Avg loss: 1.260891

Epoch 5
-------------------------------
loss: 1.337803  [   64/60000]
loss: 1.313278  [ 6464/60000]
loss: 1.151837  [12864/60000]
loss: 1.252142  [19264/60000]
loss: 1.123048  [25664/60000]
loss: 1.159531  [32064/60000]
loss: 1.175011  [38464/60000]
loss: 1.115554  [44864/60000]
loss: 1.160974  [51264/60000]
loss: 1.062730  [57664/60000]
Test Error:
 Accuracy: 64.6%, Avg loss: 1.087374

Done!
</pre>
   </div>
  </div>
  <p>
   Read more about
   <a class=""reference external"" href=""optimization_tutorial.html"">
    Training your model
   </a>
   .
  </p>
  <hr class=""docutils""/>
 </div>
 <div class=""section"" id=""saving-models"">
  <h2>
   Saving Models
   <a class=""headerlink"" href=""#saving-models"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   A common way to save a model is to serialize the internal state dictionary (containing the model parameters).
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"" title=""torch.save""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">save</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict"" title=""torch.nn.Module.state_dict""><span class=""n"">model</span><span class=""o"">.</span><span class=""n"">state_dict</span></a><span class=""p"">(),</span> <span class=""s2"">""model.pth""</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""s2"">""Saved PyTorch Model State to model.pth""</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>Saved PyTorch Model State to model.pth
</pre>
   </div>
  </div>
 </div>
 <div class=""section"" id=""loading-models"">
  <h2>
   Loading Models
   <a class=""headerlink"" href=""#loading-models"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   The process for loading a model includes re-creating the model structure and loading
the state dictionary into it.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""n"">model</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module"" title=""torch.nn.Module""><span class=""n"">NeuralNetwork</span></a><span class=""p"">()</span><span class=""o"">.</span><span class=""n"">to</span><span class=""p"">(</span><span class=""n"">device</span><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict"" title=""torch.nn.Module.load_state_dict""><span class=""n"">model</span><span class=""o"">.</span><span class=""n"">load_state_dict</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"" title=""torch.load""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">load</span></a><span class=""p"">(</span><span class=""s2"">""model.pth""</span><span class=""p"">,</span> <span class=""n"">weights_only</span><span class=""o"">=</span><span class=""kc"">True</span><span class=""p"">))</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>&lt;All keys matched successfully&gt;
</pre>
   </div>
  </div>
  <p>
   This model can now be used to make predictions.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""n"">classes</span> <span class=""o"">=</span> <span class=""p"">[</span>
    <span class=""s2"">""T-shirt/top""</span><span class=""p"">,</span>
    <span class=""s2"">""Trouser""</span><span class=""p"">,</span>
    <span class=""s2"">""Pullover""</span><span class=""p"">,</span>
    <span class=""s2"">""Dress""</span><span class=""p"">,</span>
    <span class=""s2"">""Coat""</span><span class=""p"">,</span>
    <span class=""s2"">""Sandal""</span><span class=""p"">,</span>
    <span class=""s2"">""Shirt""</span><span class=""p"">,</span>
    <span class=""s2"">""Sneaker""</span><span class=""p"">,</span>
    <span class=""s2"">""Bag""</span><span class=""p"">,</span>
    <span class=""s2"">""Ankle boot""</span><span class=""p"">,</span>
<span class=""p"">]</span>

<a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval"" title=""torch.nn.Module.eval""><span class=""n"">model</span><span class=""o"">.</span><span class=""n"">eval</span></a><span class=""p"">()</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">x</span></a><span class=""p"">,</span> <span class=""n"">y</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">test_data</span></a><span class=""p"">[</span><span class=""mi"">0</span><span class=""p"">][</span><span class=""mi"">0</span><span class=""p"">],</span> <a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">test_data</span></a><span class=""p"">[</span><span class=""mi"">0</span><span class=""p"">][</span><span class=""mi"">1</span><span class=""p"">]</span>
<span class=""k"">with</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad"" title=""torch.no_grad""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">no_grad</span></a><span class=""p"">():</span>
    <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">x</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">x</span></a><span class=""o"">.</span><span class=""n"">to</span><span class=""p"">(</span><span class=""n"">device</span><span class=""p"">)</span>
    <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">pred</span></a> <span class=""o"">=</span> <span class=""n"">model</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">x</span></a><span class=""p"">)</span>
    <span class=""n"">predicted</span><span class=""p"">,</span> <span class=""n"">actual</span> <span class=""o"">=</span> <span class=""n"">classes</span><span class=""p"">[</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">pred</span></a><span class=""p"">[</span><span class=""mi"">0</span><span class=""p"">]</span><span class=""o"">.</span><span class=""n"">argmax</span><span class=""p"">(</span><span class=""mi"">0</span><span class=""p"">)],</span> <span class=""n"">classes</span><span class=""p"">[</span><span class=""n"">y</span><span class=""p"">]</span>
    <span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s1"">'Predicted: ""</span><span class=""si"">{</span><span class=""n"">predicted</span><span class=""si"">}</span><span class=""s1"">"", Actual: ""</span><span class=""si"">{</span><span class=""n"">actual</span><span class=""si"">}</span><span class=""s1"">""'</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>Predicted: ""Ankle boot"", Actual: ""Ankle boot""
</pre>
   </div>
  </div>
  <p>
   Read more about
   <a class=""reference external"" href=""saveloadrun_tutorial.html"">
    Saving &amp; Loading your model
   </a>
   .
  </p>
  <p class=""sphx-glr-timing"">
   <strong>
    Total running time of the script:
   </strong>
   ( 0 minutes  59.718 seconds)
  </p>
  <div class=""sphx-glr-footer sphx-glr-footer-example docutils container"" id=""sphx-glr-download-beginner-basics-quickstart-tutorial-py"">
   <div class=""sphx-glr-download sphx-glr-download-python docutils container"">
    <p>
     <a class=""reference download internal"" download="""" href=""../../_downloads/51f1e1167acc0fda8f9d8fd8597ee626/quickstart_tutorial.py"">
      <code class=""xref download docutils literal notranslate"">
       <span class=""pre"">
        Download
       </span>
       <span class=""pre"">
        Python
       </span>
       <span class=""pre"">
        source
       </span>
       <span class=""pre"">
        code:
       </span>
       <span class=""pre"">
        quickstart_tutorial.py
       </span>
      </code>
     </a>
    </p>
   </div>
   <div class=""sphx-glr-download sphx-glr-download-jupyter docutils container"">
    <p>
     <a class=""reference download internal"" download="""" href=""../../_downloads/af0caf6d7af0dda755f4c9d7af9ccc2c/quickstart_tutorial.ipynb"">
      <code class=""xref download docutils literal notranslate"">
       <span class=""pre"">
        Download
       </span>
       <span class=""pre"">
        Jupyter
       </span>
       <span class=""pre"">
        notebook:
       </span>
       <span class=""pre"">
        quickstart_tutorial.ipynb
       </span>
      </code>
     </a>
    </p>
   </div>
  </div>
  <p class=""sphx-glr-signature"">
   <a class=""reference external"" href=""https://sphinx-gallery.github.io"">
    Gallery generated by Sphinx-Gallery
   </a>
  </p>
 </div>
</div>
"
Tensors,https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html,"<div class=""section"" id=""tensors"">
 <h1>
  Tensors
  <a class=""headerlink"" href=""#tensors"" title=""Permalink to this heading"">
   ¶
  </a>
 </h1>
 <p class=""date-info-last-verified"" style=""color: #6c6c6d; font-size: small;"">
  Created On: Feb 10, 2021 | Last Updated: Jan 24, 2025 | Last Verified: Nov 05, 2024
 </p>
 <p>
  Tensors are a specialized data structure that are very similar to arrays and matrices.
In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.
 </p>
 <p>
  Tensors are similar to
  <a class=""reference external"" href=""https://numpy.org/"">
   NumPy’s
  </a>
  ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors and
NumPy arrays can often share the same underlying memory, eliminating the need to copy data (see
  <a class=""reference internal"" href=""../blitz/tensor_tutorial.html#bridge-to-np-label"">
   <span class=""std std-ref"">
    Bridge with NumPy
   </span>
  </a>
  ). Tensors
are also optimized for automatic differentiation (we’ll see more about that later in the
  <a class=""reference external"" href=""autogradqs_tutorial.html"">
   Autograd
  </a>
  section). If you’re familiar with ndarrays, you’ll be right at home with the Tensor API. If not, follow along!
 </p>
 <div class=""highlight-default notranslate"">
  <div class=""highlight"">
   <pre><span></span><span class=""kn"">import</span><span class=""w""> </span><span class=""nn"">torch</span>
<span class=""kn"">import</span><span class=""w""> </span><span class=""nn"">numpy</span><span class=""w""> </span><span class=""k"">as</span><span class=""w""> </span><span class=""nn"">np</span>
</pre>
  </div>
 </div>
 <div class=""section"" id=""initializing-a-tensor"">
  <h2>
   Initializing a Tensor
   <a class=""headerlink"" href=""#initializing-a-tensor"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   Tensors can be initialized in various ways. Take a look at the following examples:
  </p>
  <p>
   <strong>
    Directly from data
   </strong>
  </p>
  <p>
   Tensors can be created directly from data. The data type is automatically inferred.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""n"">data</span> <span class=""o"">=</span> <span class=""p"">[[</span><span class=""mi"">1</span><span class=""p"">,</span> <span class=""mi"">2</span><span class=""p"">],[</span><span class=""mi"">3</span><span class=""p"">,</span> <span class=""mi"">4</span><span class=""p"">]]</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">x_data</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"" title=""torch.tensor""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">tensor</span></a><span class=""p"">(</span><span class=""n"">data</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <p>
   <strong>
    From a NumPy array
   </strong>
  </p>
  <p>
   Tensors can be created from NumPy arrays (and vice versa - see
   <a class=""reference internal"" href=""../blitz/tensor_tutorial.html#bridge-to-np-label"">
    <span class=""std std-ref"">
     Bridge with NumPy
    </span>
   </a>
   ).
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""n"">np_array</span> <span class=""o"">=</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">array</span><span class=""p"">(</span><span class=""n"">data</span><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">x_np</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.from_numpy.html#torch.from_numpy"" title=""torch.from_numpy""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">from_numpy</span></a><span class=""p"">(</span><span class=""n"">np_array</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <p>
   <strong>
    From another tensor:
   </strong>
  </p>
  <p>
   The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">x_ones</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.ones_like.html#torch.ones_like"" title=""torch.ones_like""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">ones_like</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">x_data</span></a><span class=""p"">)</span> <span class=""c1""># retains the properties of x_data</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Ones Tensor: </span><span class=""se"">\n</span><span class=""s2""> </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">x_ones</span></a><span class=""si"">}</span><span class=""s2""> </span><span class=""se"">\n</span><span class=""s2"">""</span><span class=""p"">)</span>

<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">x_rand</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.rand_like.html#torch.rand_like"" title=""torch.rand_like""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">rand_like</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">x_data</span></a><span class=""p"">,</span> <span class=""n"">dtype</span><span class=""o"">=</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype"" title=""torch.dtype""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">float</span></a><span class=""p"">)</span> <span class=""c1""># overrides the datatype of x_data</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Random Tensor: </span><span class=""se"">\n</span><span class=""s2""> </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">x_rand</span></a><span class=""si"">}</span><span class=""s2""> </span><span class=""se"">\n</span><span class=""s2"">""</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>Ones Tensor:
 tensor([[1, 1],
        [1, 1]])

Random Tensor:
 tensor([[0.8823, 0.9150],
        [0.3829, 0.9593]])
</pre>
   </div>
  </div>
  <p>
   <strong>
    With random or constant values:
   </strong>
  </p>
  <p>
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     shape
    </span>
   </code>
   is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""n"">shape</span> <span class=""o"">=</span> <span class=""p"">(</span><span class=""mi"">2</span><span class=""p"">,</span><span class=""mi"">3</span><span class=""p"">,)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">rand_tensor</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand"" title=""torch.rand""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">rand</span></a><span class=""p"">(</span><span class=""n"">shape</span><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">ones_tensor</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"" title=""torch.ones""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">ones</span></a><span class=""p"">(</span><span class=""n"">shape</span><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">zeros_tensor</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros"" title=""torch.zeros""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">zeros</span></a><span class=""p"">(</span><span class=""n"">shape</span><span class=""p"">)</span>

<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Random Tensor: </span><span class=""se"">\n</span><span class=""s2""> </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">rand_tensor</span></a><span class=""si"">}</span><span class=""s2""> </span><span class=""se"">\n</span><span class=""s2"">""</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Ones Tensor: </span><span class=""se"">\n</span><span class=""s2""> </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">ones_tensor</span></a><span class=""si"">}</span><span class=""s2""> </span><span class=""se"">\n</span><span class=""s2"">""</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Zeros Tensor: </span><span class=""se"">\n</span><span class=""s2""> </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">zeros_tensor</span></a><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>Random Tensor:
 tensor([[0.3904, 0.6009, 0.2566],
        [0.7936, 0.9408, 0.1332]])

Ones Tensor:
 tensor([[1., 1., 1.],
        [1., 1., 1.]])

Zeros Tensor:
 tensor([[0., 0., 0.],
        [0., 0., 0.]])
</pre>
   </div>
  </div>
  <hr class=""docutils""/>
 </div>
 <div class=""section"" id=""attributes-of-a-tensor"">
  <h2>
   Attributes of a Tensor
   <a class=""headerlink"" href=""#attributes-of-a-tensor"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   Tensor attributes describe their shape, datatype, and the device on which they are stored.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand"" title=""torch.rand""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">rand</span></a><span class=""p"">(</span><span class=""mi"">3</span><span class=""p"">,</span><span class=""mi"">4</span><span class=""p"">)</span>

<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Shape of tensor: </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/size.html#torch.Size"" title=""torch.Size""><span class=""n"">tensor</span><span class=""o"">.</span><span class=""n"">shape</span></a><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Datatype of tensor: </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype"" title=""torch.dtype""><span class=""n"">tensor</span><span class=""o"">.</span><span class=""n"">dtype</span></a><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Device tensor is stored on: </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensor_attributes.html#torch.device"" title=""torch.device""><span class=""n"">tensor</span><span class=""o"">.</span><span class=""n"">device</span></a><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>Shape of tensor: torch.Size([3, 4])
Datatype of tensor: torch.float32
Device tensor is stored on: cpu
</pre>
   </div>
  </div>
  <hr class=""docutils""/>
 </div>
 <div class=""section"" id=""operations-on-tensors"">
  <h2>
   Operations on Tensors
   <a class=""headerlink"" href=""#operations-on-tensors"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   Over 1200 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing,
indexing, slicing), sampling and more are
comprehensively described
   <a class=""reference external"" href=""https://pytorch.org/docs/stable/torch.html"">
    here
   </a>
   .
  </p>
  <p>
   Each of these operations can be run on the CPU and
   <a class=""reference external"" href=""https://pytorch.org/docs/stable/torch.html#accelerators"">
    Accelerator
   </a>
   such as CUDA, MPS, MTIA, or XPU. If you’re using Colab, allocate an accelerator by going to Runtime &gt; Change runtime type &gt; GPU.
  </p>
  <p>
   By default, tensors are created on the CPU. We need to explicitly move tensors to the accelerator using
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     .to
    </span>
   </code>
   method (after checking for accelerator availability). Keep in mind that copying large tensors
across devices can be expensive in terms of time and memory!
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""c1""># We move our tensor to the current accelerator if available</span>
<span class=""k"">if</span> <a class=""sphx-glr-backref-module-torch-accelerator sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.accelerator.is_available.html#torch.accelerator.is_available"" title=""torch.accelerator.is_available""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">accelerator</span><span class=""o"">.</span><span class=""n"">is_available</span></a><span class=""p"">():</span>
    <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a><span class=""o"">.</span><span class=""n"">to</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch-accelerator sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.accelerator.current_accelerator.html#torch.accelerator.current_accelerator"" title=""torch.accelerator.current_accelerator""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">accelerator</span><span class=""o"">.</span><span class=""n"">current_accelerator</span></a><span class=""p"">())</span>
</pre>
   </div>
  </div>
  <p>
   Try out some of the operations from the list.
If you’re familiar with the NumPy API, you’ll find the Tensor API a breeze to use.
  </p>
  <p>
   <strong>
    Standard numpy-like indexing and slicing:
   </strong>
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"" title=""torch.ones""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">ones</span></a><span class=""p"">(</span><span class=""mi"">4</span><span class=""p"">,</span> <span class=""mi"">4</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""First row: </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a><span class=""p"">[</span><span class=""mi"">0</span><span class=""p"">]</span><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""First column: </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a><span class=""p"">[:,</span><span class=""w""> </span><span class=""mi"">0</span><span class=""p"">]</span><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Last column: </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a><span class=""p"">[</span><span class=""o"">...</span><span class=""p"">,</span><span class=""w""> </span><span class=""o"">-</span><span class=""mi"">1</span><span class=""p"">]</span><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a><span class=""p"">[:,</span><span class=""mi"">1</span><span class=""p"">]</span> <span class=""o"">=</span> <span class=""mi"">0</span>
<span class=""nb"">print</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>First row: tensor([1., 1., 1., 1.])
First column: tensor([1., 1., 1., 1.])
Last column: tensor([1., 1., 1., 1.])
tensor([[1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.]])
</pre>
   </div>
  </div>
  <p>
   <strong>
    Joining tensors
   </strong>
   You can use
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     torch.cat
    </span>
   </code>
   to concatenate a sequence of tensors along a given dimension.
See also
   <a class=""reference external"" href=""https://pytorch.org/docs/stable/generated/torch.stack.html"">
    torch.stack
   </a>
   ,
another tensor joining operator that is subtly different from
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     torch.cat
    </span>
   </code>
   .
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">t1</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat"" title=""torch.cat""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">cat</span></a><span class=""p"">([</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a><span class=""p"">],</span> <span class=""n"">dim</span><span class=""o"">=</span><span class=""mi"">1</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">t1</span></a><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],
        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],
        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],
        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])
</pre>
   </div>
  </div>
  <p>
   <strong>
    Arithmetic operations
   </strong>
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""c1""># This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value</span>
<span class=""c1""># ``tensor.T`` returns the transpose of a tensor</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">y1</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a> <span class=""o"">@</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span><span class=""o"">.</span><span class=""n"">T</span></a>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">y2</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a><span class=""o"">.</span><span class=""n"">matmul</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span><span class=""o"">.</span><span class=""n"">T</span></a><span class=""p"">)</span>

<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">y3</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.rand_like.html#torch.rand_like"" title=""torch.rand_like""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">rand_like</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">y1</span></a><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"" title=""torch.matmul""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">matmul</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span><span class=""o"">.</span><span class=""n"">T</span></a><span class=""p"">,</span> <span class=""n"">out</span><span class=""o"">=</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">y3</span></a><span class=""p"">)</span>


<span class=""c1""># This computes the element-wise product. z1, z2, z3 will have the same value</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">z1</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a> <span class=""o"">*</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">z2</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a><span class=""o"">.</span><span class=""n"">mul</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a><span class=""p"">)</span>

<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">z3</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.rand_like.html#torch.rand_like"" title=""torch.rand_like""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">rand_like</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul"" title=""torch.mul""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">mul</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a><span class=""p"">,</span> <span class=""n"">out</span><span class=""o"">=</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">z3</span></a><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>tensor([[1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.]])
</pre>
   </div>
  </div>
  <p>
   <strong>
    Single-element tensors
   </strong>
   If you have a one-element tensor, for example by aggregating all
values of a tensor into one value, you can convert it to a Python
numerical value using
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     item()
    </span>
   </code>
   :
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">agg</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a><span class=""o"">.</span><span class=""n"">sum</span><span class=""p"">()</span>
<span class=""n"">agg_item</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">agg</span></a><span class=""o"">.</span><span class=""n"">item</span><span class=""p"">()</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""n"">agg_item</span><span class=""p"">,</span> <span class=""nb"">type</span><span class=""p"">(</span><span class=""n"">agg_item</span><span class=""p"">))</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>12.0 &lt;class 'float'&gt;
</pre>
   </div>
  </div>
  <p>
   <strong>
    In-place operations
   </strong>
   Operations that store the result into the operand are called in-place. They are denoted by a
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     _
    </span>
   </code>
   suffix.
For example:
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     x.copy_(y)
    </span>
   </code>
   ,
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     x.t_()
    </span>
   </code>
   , will change
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     x
    </span>
   </code>
   .
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""</span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a><span class=""si"">}</span><span class=""s2""> </span><span class=""se"">\n</span><span class=""s2"">""</span><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a><span class=""o"">.</span><span class=""n"">add_</span><span class=""p"">(</span><span class=""mi"">5</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">tensor</span></a><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>tensor([[1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.]])

tensor([[6., 5., 6., 6.],
        [6., 5., 6., 6.],
        [6., 5., 6., 6.],
        [6., 5., 6., 6.]])
</pre>
   </div>
  </div>
  <div class=""admonition note"">
   <p class=""admonition-title"">
    Note
   </p>
   <p>
    In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss
of history. Hence, their use is discouraged.
   </p>
  </div>
  <hr class=""docutils""/>
 </div>
 <div class=""section"" id=""bridge-with-numpy"">
  <span id=""bridge-to-np-label"">
  </span>
  <h2>
   Bridge with NumPy
   <a class=""headerlink"" href=""#bridge-with-numpy"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   Tensors on the CPU and NumPy arrays can share their underlying memory
locations, and changing one will change the other.
  </p>
  <div class=""section"" id=""tensor-to-numpy-array"">
   <h3>
    Tensor to NumPy array
    <a class=""headerlink"" href=""#tensor-to-numpy-array"" title=""Permalink to this heading"">
     ¶
    </a>
   </h3>
   <div class=""highlight-default notranslate"">
    <div class=""highlight"">
     <pre><span></span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">t</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"" title=""torch.ones""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">ones</span></a><span class=""p"">(</span><span class=""mi"">5</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""t: </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">t</span></a><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
<span class=""n"">n</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">t</span></a><span class=""o"">.</span><span class=""n"">numpy</span><span class=""p"">()</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""n: </span><span class=""si"">{</span><span class=""n"">n</span><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
</pre>
    </div>
   </div>
   <div class=""sphx-glr-script-out highlight-none notranslate"">
    <div class=""highlight"">
     <pre><span></span>t: tensor([1., 1., 1., 1., 1.])
n: [1. 1. 1. 1. 1.]
</pre>
    </div>
   </div>
   <p>
    A change in the tensor reflects in the NumPy array.
   </p>
   <div class=""highlight-default notranslate"">
    <div class=""highlight"">
     <pre><span></span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">t</span></a><span class=""o"">.</span><span class=""n"">add_</span><span class=""p"">(</span><span class=""mi"">1</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""t: </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">t</span></a><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""n: </span><span class=""si"">{</span><span class=""n"">n</span><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
</pre>
    </div>
   </div>
   <div class=""sphx-glr-script-out highlight-none notranslate"">
    <div class=""highlight"">
     <pre><span></span>t: tensor([2., 2., 2., 2., 2.])
n: [2. 2. 2. 2. 2.]
</pre>
    </div>
   </div>
  </div>
  <div class=""section"" id=""numpy-array-to-tensor"">
   <h3>
    NumPy array to Tensor
    <a class=""headerlink"" href=""#numpy-array-to-tensor"" title=""Permalink to this heading"">
     ¶
    </a>
   </h3>
   <div class=""highlight-default notranslate"">
    <div class=""highlight"">
     <pre><span></span><span class=""n"">n</span> <span class=""o"">=</span> <span class=""n"">np</span><span class=""o"">.</span><span class=""n"">ones</span><span class=""p"">(</span><span class=""mi"">5</span><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">t</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.from_numpy.html#torch.from_numpy"" title=""torch.from_numpy""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">from_numpy</span></a><span class=""p"">(</span><span class=""n"">n</span><span class=""p"">)</span>
</pre>
    </div>
   </div>
   <p>
    Changes in the NumPy array reflects in the tensor.
   </p>
   <div class=""highlight-default notranslate"">
    <div class=""highlight"">
     <pre><span></span><span class=""n"">np</span><span class=""o"">.</span><span class=""n"">add</span><span class=""p"">(</span><span class=""n"">n</span><span class=""p"">,</span> <span class=""mi"">1</span><span class=""p"">,</span> <span class=""n"">out</span><span class=""o"">=</span><span class=""n"">n</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""t: </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">t</span></a><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""n: </span><span class=""si"">{</span><span class=""n"">n</span><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
</pre>
    </div>
   </div>
   <div class=""sphx-glr-script-out highlight-none notranslate"">
    <div class=""highlight"">
     <pre><span></span>t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
n: [2. 2. 2. 2. 2.]
</pre>
    </div>
   </div>
   <p class=""sphx-glr-timing"">
    <strong>
     Total running time of the script:
    </strong>
    ( 0 minutes  0.023 seconds)
   </p>
   <div class=""sphx-glr-footer sphx-glr-footer-example docutils container"" id=""sphx-glr-download-beginner-basics-tensorqs-tutorial-py"">
    <div class=""sphx-glr-download sphx-glr-download-python docutils container"">
     <p>
      <a class=""reference download internal"" download="""" href=""../../_downloads/3fb82dc8278b08d5e5dee31ec1c16170/tensorqs_tutorial.py"">
       <code class=""xref download docutils literal notranslate"">
        <span class=""pre"">
         Download
        </span>
        <span class=""pre"">
         Python
        </span>
        <span class=""pre"">
         source
        </span>
        <span class=""pre"">
         code:
        </span>
        <span class=""pre"">
         tensorqs_tutorial.py
        </span>
       </code>
      </a>
     </p>
    </div>
    <div class=""sphx-glr-download sphx-glr-download-jupyter docutils container"">
     <p>
      <a class=""reference download internal"" download="""" href=""../../_downloads/0e6615c5a7bc71e01ff3c51217ea00da/tensorqs_tutorial.ipynb"">
       <code class=""xref download docutils literal notranslate"">
        <span class=""pre"">
         Download
        </span>
        <span class=""pre"">
         Jupyter
        </span>
        <span class=""pre"">
         notebook:
        </span>
        <span class=""pre"">
         tensorqs_tutorial.ipynb
        </span>
       </code>
      </a>
     </p>
    </div>
   </div>
   <p class=""sphx-glr-signature"">
    <a class=""reference external"" href=""https://sphinx-gallery.github.io"">
     Gallery generated by Sphinx-Gallery
    </a>
   </p>
  </div>
 </div>
</div>
"
Datasets and DataLoaders,https://pytorch.org/tutorials/beginner/basics/data_tutorial.html,"<div class=""section"" id=""datasets-dataloaders"">
 <h1>
  Datasets &amp; DataLoaders
  <a class=""headerlink"" href=""#datasets-dataloaders"" title=""Permalink to this heading"">
   ¶
  </a>
 </h1>
 <p class=""date-info-last-verified"" style=""color: #6c6c6d; font-size: small;"">
  Created On: Feb 09, 2021 | Last Updated: Jan 16, 2024 | Last Verified: Nov 05, 2024
 </p>
 <p>
  Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code
to be decoupled from our model training code for better readability and modularity.
PyTorch provides two data primitives:
  <code class=""docutils literal notranslate"">
   <span class=""pre"">
    torch.utils.data.DataLoader
   </span>
  </code>
  and
  <code class=""docutils literal notranslate"">
   <span class=""pre"">
    torch.utils.data.Dataset
   </span>
  </code>
  that allow you to use pre-loaded datasets as well as your own data.
  <code class=""docutils literal notranslate"">
   <span class=""pre"">
    Dataset
   </span>
  </code>
  stores the samples and their corresponding labels, and
  <code class=""docutils literal notranslate"">
   <span class=""pre"">
    DataLoader
   </span>
  </code>
  wraps an iterable around
the
  <code class=""docutils literal notranslate"">
   <span class=""pre"">
    Dataset
   </span>
  </code>
  to enable easy access to the samples.
 </p>
 <p>
  PyTorch domain libraries provide a number of pre-loaded datasets (such as FashionMNIST) that
subclass
  <code class=""docutils literal notranslate"">
   <span class=""pre"">
    torch.utils.data.Dataset
   </span>
  </code>
  and implement functions specific to the particular data.
They can be used to prototype and benchmark your model. You can find them
here:
  <a class=""reference external"" href=""https://pytorch.org/vision/stable/datasets.html"">
   Image Datasets
  </a>
  ,
  <a class=""reference external"" href=""https://pytorch.org/text/stable/datasets.html"">
   Text Datasets
  </a>
  , and
  <a class=""reference external"" href=""https://pytorch.org/audio/stable/datasets.html"">
   Audio Datasets
  </a>
 </p>
 <div class=""section"" id=""loading-a-dataset"">
  <h2>
   Loading a Dataset
   <a class=""headerlink"" href=""#loading-a-dataset"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   Here is an example of how to load the
   <a class=""reference external"" href=""https://research.zalando.com/project/fashion_mnist/fashion_mnist/"">
    Fashion-MNIST
   </a>
   dataset from TorchVision.
Fashion-MNIST is a dataset of Zalando’s article images consisting of 60,000 training examples and 10,000 test examples.
Each example comprises a 28×28 grayscale image and an associated label from one of 10 classes.
  </p>
  <dl class=""simple"">
   <dt>
    We load the
    <a class=""reference external"" href=""https://pytorch.org/vision/stable/datasets.html#fashion-mnist"">
     FashionMNIST Dataset
    </a>
    with the following parameters:
   </dt>
   <dd>
    <ul class=""simple"">
     <li>
      <p>
       <code class=""docutils literal notranslate"">
        <span class=""pre"">
         root
        </span>
       </code>
       is the path where the train/test data is stored,
      </p>
     </li>
     <li>
      <p>
       <code class=""docutils literal notranslate"">
        <span class=""pre"">
         train
        </span>
       </code>
       specifies training or test dataset,
      </p>
     </li>
     <li>
      <p>
       <code class=""docutils literal notranslate"">
        <span class=""pre"">
         download=True
        </span>
       </code>
       downloads the data from the internet if it’s not available at
       <code class=""docutils literal notranslate"">
        <span class=""pre"">
         root
        </span>
       </code>
       .
      </p>
     </li>
     <li>
      <p>
       <code class=""docutils literal notranslate"">
        <span class=""pre"">
         transform
        </span>
       </code>
       and
       <code class=""docutils literal notranslate"">
        <span class=""pre"">
         target_transform
        </span>
       </code>
       specify the feature and label transformations
      </p>
     </li>
    </ul>
   </dd>
  </dl>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""kn"">import</span><span class=""w""> </span><span class=""nn"">torch</span>
<span class=""kn"">from</span><span class=""w""> </span><span class=""nn"">torch.utils.data</span><span class=""w""> </span><span class=""kn"">import</span> <a class=""sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"" title=""torch.utils.data.Dataset""><span class=""n"">Dataset</span></a>
<span class=""kn"">from</span><span class=""w""> </span><span class=""nn"">torchvision</span><span class=""w""> </span><span class=""kn"">import</span> <span class=""n"">datasets</span>
<span class=""kn"">from</span><span class=""w""> </span><span class=""nn"">torchvision.transforms</span><span class=""w""> </span><span class=""kn"">import</span> <a class=""sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class"" href=""https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor"" title=""torchvision.transforms.ToTensor""><span class=""n"">ToTensor</span></a>
<span class=""kn"">import</span><span class=""w""> </span><span class=""nn"">matplotlib.pyplot</span><span class=""w""> </span><span class=""k"">as</span><span class=""w""> </span><span class=""nn"">plt</span>


<a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">training_data</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">datasets</span><span class=""o"">.</span><span class=""n"">FashionMNIST</span></a><span class=""p"">(</span>
    <span class=""n"">root</span><span class=""o"">=</span><span class=""s2"">""data""</span><span class=""p"">,</span>
    <span class=""n"">train</span><span class=""o"">=</span><span class=""kc"">True</span><span class=""p"">,</span>
    <span class=""n"">download</span><span class=""o"">=</span><span class=""kc"">True</span><span class=""p"">,</span>
    <span class=""n"">transform</span><span class=""o"">=</span><a class=""sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class"" href=""https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor"" title=""torchvision.transforms.ToTensor""><span class=""n"">ToTensor</span></a><span class=""p"">()</span>
<span class=""p"">)</span>

<a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">test_data</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">datasets</span><span class=""o"">.</span><span class=""n"">FashionMNIST</span></a><span class=""p"">(</span>
    <span class=""n"">root</span><span class=""o"">=</span><span class=""s2"">""data""</span><span class=""p"">,</span>
    <span class=""n"">train</span><span class=""o"">=</span><span class=""kc"">False</span><span class=""p"">,</span>
    <span class=""n"">download</span><span class=""o"">=</span><span class=""kc"">True</span><span class=""p"">,</span>
    <span class=""n"">transform</span><span class=""o"">=</span><a class=""sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class"" href=""https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor"" title=""torchvision.transforms.ToTensor""><span class=""n"">ToTensor</span></a><span class=""p"">()</span>
<span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>  0%|          | 0.00/26.4M [00:00&lt;?, ?B/s]
  0%|          | 65.5k/26.4M [00:00&lt;01:12, 365kB/s]
  1%|          | 229k/26.4M [00:00&lt;00:37, 691kB/s]
  4%|3         | 950k/26.4M [00:00&lt;00:11, 2.21MB/s]
 15%|#4        | 3.83M/26.4M [00:00&lt;00:02, 7.67MB/s]
 37%|###7      | 9.86M/26.4M [00:00&lt;00:00, 17.0MB/s]
 60%|######    | 15.9M/26.4M [00:00&lt;00:00, 26.7MB/s]
 72%|#######2  | 19.1M/26.4M [00:01&lt;00:00, 23.8MB/s]
 94%|#########3| 24.8M/26.4M [00:01&lt;00:00, 31.4MB/s]
100%|##########| 26.4M/26.4M [00:01&lt;00:00, 19.5MB/s]

  0%|          | 0.00/29.5k [00:00&lt;?, ?B/s]
100%|##########| 29.5k/29.5k [00:00&lt;00:00, 324kB/s]

  0%|          | 0.00/4.42M [00:00&lt;?, ?B/s]
  1%|1         | 65.5k/4.42M [00:00&lt;00:12, 361kB/s]
  5%|5         | 229k/4.42M [00:00&lt;00:06, 680kB/s]
 21%|##        | 918k/4.42M [00:00&lt;00:01, 2.59MB/s]
 44%|####3     | 1.93M/4.42M [00:00&lt;00:00, 4.08MB/s]
100%|##########| 4.42M/4.42M [00:00&lt;00:00, 6.06MB/s]

  0%|          | 0.00/5.15k [00:00&lt;?, ?B/s]
100%|##########| 5.15k/5.15k [00:00&lt;00:00, 44.3MB/s]
</pre>
   </div>
  </div>
 </div>
 <div class=""section"" id=""iterating-and-visualizing-the-dataset"">
  <h2>
   Iterating and Visualizing the Dataset
   <a class=""headerlink"" href=""#iterating-and-visualizing-the-dataset"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   We can index
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     Datasets
    </span>
   </code>
   manually like a list:
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     training_data[index]
    </span>
   </code>
   .
We use
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     matplotlib
    </span>
   </code>
   to visualize some samples in our training data.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""n"">labels_map</span> <span class=""o"">=</span> <span class=""p"">{</span>
    <span class=""mi"">0</span><span class=""p"">:</span> <span class=""s2"">""T-Shirt""</span><span class=""p"">,</span>
    <span class=""mi"">1</span><span class=""p"">:</span> <span class=""s2"">""Trouser""</span><span class=""p"">,</span>
    <span class=""mi"">2</span><span class=""p"">:</span> <span class=""s2"">""Pullover""</span><span class=""p"">,</span>
    <span class=""mi"">3</span><span class=""p"">:</span> <span class=""s2"">""Dress""</span><span class=""p"">,</span>
    <span class=""mi"">4</span><span class=""p"">:</span> <span class=""s2"">""Coat""</span><span class=""p"">,</span>
    <span class=""mi"">5</span><span class=""p"">:</span> <span class=""s2"">""Sandal""</span><span class=""p"">,</span>
    <span class=""mi"">6</span><span class=""p"">:</span> <span class=""s2"">""Shirt""</span><span class=""p"">,</span>
    <span class=""mi"">7</span><span class=""p"">:</span> <span class=""s2"">""Sneaker""</span><span class=""p"">,</span>
    <span class=""mi"">8</span><span class=""p"">:</span> <span class=""s2"">""Bag""</span><span class=""p"">,</span>
    <span class=""mi"">9</span><span class=""p"">:</span> <span class=""s2"">""Ankle Boot""</span><span class=""p"">,</span>
<span class=""p"">}</span>
<span class=""n"">figure</span> <span class=""o"">=</span> <span class=""n"">plt</span><span class=""o"">.</span><span class=""n"">figure</span><span class=""p"">(</span><span class=""n"">figsize</span><span class=""o"">=</span><span class=""p"">(</span><span class=""mi"">8</span><span class=""p"">,</span> <span class=""mi"">8</span><span class=""p"">))</span>
<span class=""n"">cols</span><span class=""p"">,</span> <span class=""n"">rows</span> <span class=""o"">=</span> <span class=""mi"">3</span><span class=""p"">,</span> <span class=""mi"">3</span>
<span class=""k"">for</span> <span class=""n"">i</span> <span class=""ow"">in</span> <span class=""nb"">range</span><span class=""p"">(</span><span class=""mi"">1</span><span class=""p"">,</span> <span class=""n"">cols</span> <span class=""o"">*</span> <span class=""n"">rows</span> <span class=""o"">+</span> <span class=""mi"">1</span><span class=""p"">):</span>
    <span class=""n"">sample_idx</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint"" title=""torch.randint""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">randint</span></a><span class=""p"">(</span><span class=""nb"">len</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">training_data</span></a><span class=""p"">),</span> <span class=""n"">size</span><span class=""o"">=</span><span class=""p"">(</span><span class=""mi"">1</span><span class=""p"">,))</span><span class=""o"">.</span><span class=""n"">item</span><span class=""p"">()</span>
    <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">img</span></a><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">label</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">training_data</span></a><span class=""p"">[</span><span class=""n"">sample_idx</span><span class=""p"">]</span>
    <span class=""n"">figure</span><span class=""o"">.</span><span class=""n"">add_subplot</span><span class=""p"">(</span><span class=""n"">rows</span><span class=""p"">,</span> <span class=""n"">cols</span><span class=""p"">,</span> <span class=""n"">i</span><span class=""p"">)</span>
    <span class=""n"">plt</span><span class=""o"">.</span><span class=""n"">title</span><span class=""p"">(</span><span class=""n"">labels_map</span><span class=""p"">[</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">label</span></a><span class=""p"">])</span>
    <span class=""n"">plt</span><span class=""o"">.</span><span class=""n"">axis</span><span class=""p"">(</span><span class=""s2"">""off""</span><span class=""p"">)</span>
    <span class=""n"">plt</span><span class=""o"">.</span><span class=""n"">imshow</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">img</span></a><span class=""o"">.</span><span class=""n"">squeeze</span><span class=""p"">(),</span> <span class=""n"">cmap</span><span class=""o"">=</span><span class=""s2"">""gray""</span><span class=""p"">)</span>
<span class=""n"">plt</span><span class=""o"">.</span><span class=""n"">show</span><span class=""p"">()</span>
</pre>
   </div>
  </div>
  <img alt=""Ankle Boot, Shirt, Bag, Ankle Boot, Trouser, Sandal, Coat, Sandal, Pullover"" class=""sphx-glr-single-img"" src=""https://pytorch.org/tutorials/_images/sphx_glr_data_tutorial_001.png"" srcset=""../../_images/sphx_glr_data_tutorial_001.png""/>
  <hr class=""docutils""/>
 </div>
 <div class=""section"" id=""creating-a-custom-dataset-for-your-files"">
  <h2>
   Creating a Custom Dataset for your files
   <a class=""headerlink"" href=""#creating-a-custom-dataset-for-your-files"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   A custom Dataset class must implement three functions:
   <cite>
    __init__
   </cite>
   ,
   <cite>
    __len__
   </cite>
   , and
   <cite>
    __getitem__
   </cite>
   .
Take a look at this implementation; the FashionMNIST images are stored
in a directory
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     img_dir
    </span>
   </code>
   , and their labels are stored separately in a CSV file
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     annotations_file
    </span>
   </code>
   .
  </p>
  <p>
   In the next sections, we’ll break down what’s happening in each of these functions.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""kn"">import</span><span class=""w""> </span><span class=""nn"">os</span>
<span class=""kn"">import</span><span class=""w""> </span><span class=""nn"">pandas</span><span class=""w""> </span><span class=""k"">as</span><span class=""w""> </span><span class=""nn"">pd</span>
<span class=""kn"">from</span><span class=""w""> </span><span class=""nn"">torchvision.io</span><span class=""w""> </span><span class=""kn"">import</span> <a class=""sphx-glr-backref-module-torchvision-io sphx-glr-backref-type-py-function"" href=""https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image"" title=""torchvision.io.read_image""><span class=""n"">read_image</span></a>

<span class=""k"">class</span><span class=""w""> </span><span class=""nc"">CustomImageDataset</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"" title=""torch.utils.data.Dataset""><span class=""n"">Dataset</span></a><span class=""p"">):</span>
    <span class=""k"">def</span><span class=""w""> </span><span class=""fm"">__init__</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">,</span> <span class=""n"">annotations_file</span><span class=""p"">,</span> <span class=""n"">img_dir</span><span class=""p"">,</span> <span class=""n"">transform</span><span class=""o"">=</span><span class=""kc"">None</span><span class=""p"">,</span> <span class=""n"">target_transform</span><span class=""o"">=</span><span class=""kc"">None</span><span class=""p"">):</span>
        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">img_labels</span> <span class=""o"">=</span> <span class=""n"">pd</span><span class=""o"">.</span><span class=""n"">read_csv</span><span class=""p"">(</span><span class=""n"">annotations_file</span><span class=""p"">)</span>
        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">img_dir</span> <span class=""o"">=</span> <span class=""n"">img_dir</span>
        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">transform</span> <span class=""o"">=</span> <span class=""n"">transform</span>
        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">target_transform</span> <span class=""o"">=</span> <span class=""n"">target_transform</span>

    <span class=""k"">def</span><span class=""w""> </span><span class=""fm"">__len__</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">):</span>
        <span class=""k"">return</span> <span class=""nb"">len</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">img_labels</span><span class=""p"">)</span>

    <span class=""k"">def</span><span class=""w""> </span><span class=""fm"">__getitem__</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">,</span> <span class=""n"">idx</span><span class=""p"">):</span>
        <span class=""n"">img_path</span> <span class=""o"">=</span> <span class=""n"">os</span><span class=""o"">.</span><span class=""n"">path</span><span class=""o"">.</span><span class=""n"">join</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">img_dir</span><span class=""p"">,</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">img_labels</span><span class=""o"">.</span><span class=""n"">iloc</span><span class=""p"">[</span><span class=""n"">idx</span><span class=""p"">,</span> <span class=""mi"">0</span><span class=""p"">])</span>
        <span class=""n"">image</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torchvision-io sphx-glr-backref-type-py-function"" href=""https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image"" title=""torchvision.io.read_image""><span class=""n"">read_image</span></a><span class=""p"">(</span><span class=""n"">img_path</span><span class=""p"">)</span>
        <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">label</span></a> <span class=""o"">=</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">img_labels</span><span class=""o"">.</span><span class=""n"">iloc</span><span class=""p"">[</span><span class=""n"">idx</span><span class=""p"">,</span> <span class=""mi"">1</span><span class=""p"">]</span>
        <span class=""k"">if</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">transform</span><span class=""p"">:</span>
            <span class=""n"">image</span> <span class=""o"">=</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">transform</span><span class=""p"">(</span><span class=""n"">image</span><span class=""p"">)</span>
        <span class=""k"">if</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">target_transform</span><span class=""p"">:</span>
            <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">label</span></a> <span class=""o"">=</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">target_transform</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">label</span></a><span class=""p"">)</span>
        <span class=""k"">return</span> <span class=""n"">image</span><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">label</span></a>
</pre>
   </div>
  </div>
  <div class=""section"" id=""init"">
   <h3>
    <code class=""docutils literal notranslate"">
     <span class=""pre"">
      __init__
     </span>
    </code>
    <a class=""headerlink"" href=""#init"" title=""Permalink to this heading"">
     ¶
    </a>
   </h3>
   <p>
    The __init__ function is run once when instantiating the Dataset object. We initialize
the directory containing the images, the annotations file, and both transforms (covered
in more detail in the next section).
   </p>
   <p>
    The labels.csv file looks like:
   </p>
   <div class=""highlight-default notranslate"">
    <div class=""highlight"">
     <pre><span></span><span class=""n"">tshirt1</span><span class=""o"">.</span><span class=""n"">jpg</span><span class=""p"">,</span> <span class=""mi"">0</span>
<span class=""n"">tshirt2</span><span class=""o"">.</span><span class=""n"">jpg</span><span class=""p"">,</span> <span class=""mi"">0</span>
<span class=""o"">......</span>
<span class=""n"">ankleboot999</span><span class=""o"">.</span><span class=""n"">jpg</span><span class=""p"">,</span> <span class=""mi"">9</span>
</pre>
    </div>
   </div>
   <div class=""highlight-default notranslate"">
    <div class=""highlight"">
     <pre><span></span><span class=""k"">def</span><span class=""w""> </span><span class=""fm"">__init__</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">,</span> <span class=""n"">annotations_file</span><span class=""p"">,</span> <span class=""n"">img_dir</span><span class=""p"">,</span> <span class=""n"">transform</span><span class=""o"">=</span><span class=""kc"">None</span><span class=""p"">,</span> <span class=""n"">target_transform</span><span class=""o"">=</span><span class=""kc"">None</span><span class=""p"">):</span>
    <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">img_labels</span> <span class=""o"">=</span> <span class=""n"">pd</span><span class=""o"">.</span><span class=""n"">read_csv</span><span class=""p"">(</span><span class=""n"">annotations_file</span><span class=""p"">)</span>
    <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">img_dir</span> <span class=""o"">=</span> <span class=""n"">img_dir</span>
    <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">transform</span> <span class=""o"">=</span> <span class=""n"">transform</span>
    <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">target_transform</span> <span class=""o"">=</span> <span class=""n"">target_transform</span>
</pre>
    </div>
   </div>
  </div>
  <div class=""section"" id=""len"">
   <h3>
    <code class=""docutils literal notranslate"">
     <span class=""pre"">
      __len__
     </span>
    </code>
    <a class=""headerlink"" href=""#len"" title=""Permalink to this heading"">
     ¶
    </a>
   </h3>
   <p>
    The __len__ function returns the number of samples in our dataset.
   </p>
   <p>
    Example:
   </p>
   <div class=""highlight-default notranslate"">
    <div class=""highlight"">
     <pre><span></span><span class=""k"">def</span><span class=""w""> </span><span class=""fm"">__len__</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">):</span>
    <span class=""k"">return</span> <span class=""nb"">len</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">img_labels</span><span class=""p"">)</span>
</pre>
    </div>
   </div>
  </div>
  <div class=""section"" id=""getitem"">
   <h3>
    <code class=""docutils literal notranslate"">
     <span class=""pre"">
      __getitem__
     </span>
    </code>
    <a class=""headerlink"" href=""#getitem"" title=""Permalink to this heading"">
     ¶
    </a>
   </h3>
   <p>
    The __getitem__ function loads and returns a sample from the dataset at the given index
    <code class=""docutils literal notranslate"">
     <span class=""pre"">
      idx
     </span>
    </code>
    .
Based on the index, it identifies the image’s location on disk, converts that to a tensor using
    <code class=""docutils literal notranslate"">
     <span class=""pre"">
      read_image
     </span>
    </code>
    , retrieves the
corresponding label from the csv data in
    <code class=""docutils literal notranslate"">
     <span class=""pre"">
      self.img_labels
     </span>
    </code>
    , calls the transform functions on them (if applicable), and returns the
tensor image and corresponding label in a tuple.
   </p>
   <div class=""highlight-default notranslate"">
    <div class=""highlight"">
     <pre><span></span><span class=""k"">def</span><span class=""w""> </span><span class=""fm"">__getitem__</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">,</span> <span class=""n"">idx</span><span class=""p"">):</span>
    <span class=""n"">img_path</span> <span class=""o"">=</span> <span class=""n"">os</span><span class=""o"">.</span><span class=""n"">path</span><span class=""o"">.</span><span class=""n"">join</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">img_dir</span><span class=""p"">,</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">img_labels</span><span class=""o"">.</span><span class=""n"">iloc</span><span class=""p"">[</span><span class=""n"">idx</span><span class=""p"">,</span> <span class=""mi"">0</span><span class=""p"">])</span>
    <span class=""n"">image</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torchvision-io sphx-glr-backref-type-py-function"" href=""https://pytorch.org/vision/stable/generated/torchvision.io.read_image.html#torchvision.io.read_image"" title=""torchvision.io.read_image""><span class=""n"">read_image</span></a><span class=""p"">(</span><span class=""n"">img_path</span><span class=""p"">)</span>
    <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">label</span></a> <span class=""o"">=</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">img_labels</span><span class=""o"">.</span><span class=""n"">iloc</span><span class=""p"">[</span><span class=""n"">idx</span><span class=""p"">,</span> <span class=""mi"">1</span><span class=""p"">]</span>
    <span class=""k"">if</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">transform</span><span class=""p"">:</span>
        <span class=""n"">image</span> <span class=""o"">=</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">transform</span><span class=""p"">(</span><span class=""n"">image</span><span class=""p"">)</span>
    <span class=""k"">if</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">target_transform</span><span class=""p"">:</span>
        <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">label</span></a> <span class=""o"">=</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">target_transform</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">label</span></a><span class=""p"">)</span>
    <span class=""k"">return</span> <span class=""n"">image</span><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">label</span></a>
</pre>
    </div>
   </div>
   <hr class=""docutils""/>
  </div>
 </div>
 <div class=""section"" id=""preparing-your-data-for-training-with-dataloaders"">
  <h2>
   Preparing your data for training with DataLoaders
   <a class=""headerlink"" href=""#preparing-your-data-for-training-with-dataloaders"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   The
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     Dataset
    </span>
   </code>
   retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to
pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     multiprocessing
    </span>
   </code>
   to
speed up data retrieval.
  </p>
  <p>
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     DataLoader
    </span>
   </code>
   is an iterable that abstracts this complexity for us in an easy API.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""kn"">from</span><span class=""w""> </span><span class=""nn"">torch.utils.data</span><span class=""w""> </span><span class=""kn"">import</span> <a class=""sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" title=""torch.utils.data.DataLoader""><span class=""n"">DataLoader</span></a>

<a class=""sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" title=""torch.utils.data.DataLoader""><span class=""n"">train_dataloader</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" title=""torch.utils.data.DataLoader""><span class=""n"">DataLoader</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">training_data</span></a><span class=""p"">,</span> <span class=""n"">batch_size</span><span class=""o"">=</span><span class=""mi"">64</span><span class=""p"">,</span> <span class=""n"">shuffle</span><span class=""o"">=</span><span class=""kc"">True</span><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" title=""torch.utils.data.DataLoader""><span class=""n"">test_dataloader</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" title=""torch.utils.data.DataLoader""><span class=""n"">DataLoader</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">test_data</span></a><span class=""p"">,</span> <span class=""n"">batch_size</span><span class=""o"">=</span><span class=""mi"">64</span><span class=""p"">,</span> <span class=""n"">shuffle</span><span class=""o"">=</span><span class=""kc"">True</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
 </div>
 <div class=""section"" id=""iterate-through-the-dataloader"">
  <h2>
   Iterate through the DataLoader
   <a class=""headerlink"" href=""#iterate-through-the-dataloader"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   We have loaded that dataset into the
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     DataLoader
    </span>
   </code>
   and can iterate through the dataset as needed.
Each iteration below returns a batch of
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     train_features
    </span>
   </code>
   and
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     train_labels
    </span>
   </code>
   (containing
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     batch_size=64
    </span>
   </code>
   features and labels respectively).
Because we specified
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     shuffle=True
    </span>
   </code>
   , after we iterate over all batches the data is shuffled (for finer-grained control over
the data loading order, take a look at
   <a class=""reference external"" href=""https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler"">
    Samplers
   </a>
   ).
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""c1""># Display image and label.</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">train_features</span></a><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">train_labels</span></a> <span class=""o"">=</span> <span class=""nb"">next</span><span class=""p"">(</span><span class=""nb"">iter</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" title=""torch.utils.data.DataLoader""><span class=""n"">train_dataloader</span></a><span class=""p"">))</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Feature batch shape: </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">train_features</span></a><span class=""o"">.</span><span class=""n"">size</span><span class=""p"">()</span><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Labels batch shape: </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">train_labels</span></a><span class=""o"">.</span><span class=""n"">size</span><span class=""p"">()</span><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">img</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">train_features</span></a><span class=""p"">[</span><span class=""mi"">0</span><span class=""p"">]</span><span class=""o"">.</span><span class=""n"">squeeze</span><span class=""p"">()</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">label</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">train_labels</span></a><span class=""p"">[</span><span class=""mi"">0</span><span class=""p"">]</span>
<span class=""n"">plt</span><span class=""o"">.</span><span class=""n"">imshow</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">img</span></a><span class=""p"">,</span> <span class=""n"">cmap</span><span class=""o"">=</span><span class=""s2"">""gray""</span><span class=""p"">)</span>
<span class=""n"">plt</span><span class=""o"">.</span><span class=""n"">show</span><span class=""p"">()</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Label: </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">label</span></a><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <img alt=""data tutorial"" class=""sphx-glr-single-img"" src=""https://pytorch.org/tutorials/_images/sphx_glr_data_tutorial_002.png"" srcset=""../../_images/sphx_glr_data_tutorial_002.png""/>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>Feature batch shape: torch.Size([64, 1, 28, 28])
Labels batch shape: torch.Size([64])
Label: 5
</pre>
   </div>
  </div>
  <hr class=""docutils""/>
 </div>
 <div class=""section"" id=""further-reading"">
  <h2>
   Further Reading
   <a class=""headerlink"" href=""#further-reading"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <ul class=""simple"">
   <li>
    <p>
     <a class=""reference external"" href=""https://pytorch.org/docs/stable/data.html"">
      torch.utils.data API
     </a>
    </p>
   </li>
  </ul>
  <p class=""sphx-glr-timing"">
   <strong>
    Total running time of the script:
   </strong>
   ( 0 minutes  5.268 seconds)
  </p>
  <div class=""sphx-glr-footer sphx-glr-footer-example docutils container"" id=""sphx-glr-download-beginner-basics-data-tutorial-py"">
   <div class=""sphx-glr-download sphx-glr-download-python docutils container"">
    <p>
     <a class=""reference download internal"" download="""" href=""../../_downloads/56e3f440fc204e02856f8889c226d2d1/data_tutorial.py"">
      <code class=""xref download docutils literal notranslate"">
       <span class=""pre"">
        Download
       </span>
       <span class=""pre"">
        Python
       </span>
       <span class=""pre"">
        source
       </span>
       <span class=""pre"">
        code:
       </span>
       <span class=""pre"">
        data_tutorial.py
       </span>
      </code>
     </a>
    </p>
   </div>
   <div class=""sphx-glr-download sphx-glr-download-jupyter docutils container"">
    <p>
     <a class=""reference download internal"" download="""" href=""../../_downloads/36608d2d57f623ba3a623e0c947a8c3e/data_tutorial.ipynb"">
      <code class=""xref download docutils literal notranslate"">
       <span class=""pre"">
        Download
       </span>
       <span class=""pre"">
        Jupyter
       </span>
       <span class=""pre"">
        notebook:
       </span>
       <span class=""pre"">
        data_tutorial.ipynb
       </span>
      </code>
     </a>
    </p>
   </div>
  </div>
  <p class=""sphx-glr-signature"">
   <a class=""reference external"" href=""https://sphinx-gallery.github.io"">
    Gallery generated by Sphinx-Gallery
   </a>
  </p>
 </div>
</div>
"
Transforms,https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html,"<div class=""section"" id=""transforms"">
 <h1>
  Transforms
  <a class=""headerlink"" href=""#transforms"" title=""Permalink to this heading"">
   ¶
  </a>
 </h1>
 <p class=""date-info-last-verified"" style=""color: #6c6c6d; font-size: small;"">
  Created On: Feb 09, 2021 | Last Updated: Aug 11, 2021 | Last Verified: Not Verified
 </p>
 <p>
  Data does not always come in its final processed form that is required for
training machine learning algorithms. We use
  <strong>
   transforms
  </strong>
  to perform some
manipulation of the data and make it suitable for training.
 </p>
 <p>
  All TorchVision datasets have two parameters -
  <code class=""docutils literal notranslate"">
   <span class=""pre"">
    transform
   </span>
  </code>
  to modify the features and
  <code class=""docutils literal notranslate"">
   <span class=""pre"">
    target_transform
   </span>
  </code>
  to modify the labels - that accept callables containing the transformation logic.
The
  <a class=""reference external"" href=""https://pytorch.org/vision/stable/transforms.html"">
   torchvision.transforms
  </a>
  module offers
several commonly-used transforms out of the box.
 </p>
 <p>
  The FashionMNIST features are in PIL Image format, and the labels are integers.
For training, we need the features as normalized tensors, and the labels as one-hot encoded tensors.
To make these transformations, we use
  <code class=""docutils literal notranslate"">
   <span class=""pre"">
    ToTensor
   </span>
  </code>
  and
  <code class=""docutils literal notranslate"">
   <span class=""pre"">
    Lambda
   </span>
  </code>
  .
 </p>
 <div class=""highlight-default notranslate"">
  <div class=""highlight"">
   <pre><span></span><span class=""kn"">import</span><span class=""w""> </span><span class=""nn"">torch</span>
<span class=""kn"">from</span><span class=""w""> </span><span class=""nn"">torchvision</span><span class=""w""> </span><span class=""kn"">import</span> <span class=""n"">datasets</span>
<span class=""kn"">from</span><span class=""w""> </span><span class=""nn"">torchvision.transforms</span><span class=""w""> </span><span class=""kn"">import</span> <a class=""sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class"" href=""https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor"" title=""torchvision.transforms.ToTensor""><span class=""n"">ToTensor</span></a><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class"" href=""https://pytorch.org/vision/stable/generated/torchvision.transforms.Lambda.html#torchvision.transforms.Lambda"" title=""torchvision.transforms.Lambda""><span class=""n"">Lambda</span></a>

<a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">ds</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">datasets</span><span class=""o"">.</span><span class=""n"">FashionMNIST</span></a><span class=""p"">(</span>
    <span class=""n"">root</span><span class=""o"">=</span><span class=""s2"">""data""</span><span class=""p"">,</span>
    <span class=""n"">train</span><span class=""o"">=</span><span class=""kc"">True</span><span class=""p"">,</span>
    <span class=""n"">download</span><span class=""o"">=</span><span class=""kc"">True</span><span class=""p"">,</span>
    <span class=""n"">transform</span><span class=""o"">=</span><a class=""sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class"" href=""https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor"" title=""torchvision.transforms.ToTensor""><span class=""n"">ToTensor</span></a><span class=""p"">(),</span>
    <a class=""sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/vision/stable/generated/torchvision.transforms.Lambda.html#torchvision.transforms.Lambda"" title=""torchvision.transforms.Lambda""><span class=""n"">target_transform</span></a><span class=""o"">=</span><a class=""sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class"" href=""https://pytorch.org/vision/stable/generated/torchvision.transforms.Lambda.html#torchvision.transforms.Lambda"" title=""torchvision.transforms.Lambda""><span class=""n"">Lambda</span></a><span class=""p"">(</span><span class=""k"">lambda</span> <span class=""n"">y</span><span class=""p"">:</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros"" title=""torch.zeros""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">zeros</span></a><span class=""p"">(</span><span class=""mi"">10</span><span class=""p"">,</span> <span class=""n"">dtype</span><span class=""o"">=</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype"" title=""torch.dtype""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">float</span></a><span class=""p"">)</span><span class=""o"">.</span><span class=""n"">scatter_</span><span class=""p"">(</span><span class=""mi"">0</span><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"" title=""torch.tensor""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">tensor</span></a><span class=""p"">(</span><span class=""n"">y</span><span class=""p"">),</span> <span class=""n"">value</span><span class=""o"">=</span><span class=""mi"">1</span><span class=""p"">))</span>
<span class=""p"">)</span>
</pre>
  </div>
 </div>
 <div class=""sphx-glr-script-out highlight-none notranslate"">
  <div class=""highlight"">
   <pre><span></span>  0%|          | 0.00/26.4M [00:00&lt;?, ?B/s]
  0%|          | 65.5k/26.4M [00:00&lt;01:13, 360kB/s]
  1%|          | 229k/26.4M [00:00&lt;00:38, 677kB/s]
  3%|3         | 918k/26.4M [00:00&lt;00:09, 2.60MB/s]
  7%|7         | 1.93M/26.4M [00:00&lt;00:06, 4.05MB/s]
 13%|#3        | 3.54M/26.4M [00:00&lt;00:03, 7.13MB/s]
 32%|###2      | 8.55M/26.4M [00:00&lt;00:00, 18.4MB/s]
 44%|####3     | 11.5M/26.4M [00:01&lt;00:00, 18.9MB/s]
 54%|#####3    | 14.2M/26.4M [00:01&lt;00:00, 20.5MB/s]
 71%|#######   | 18.7M/26.4M [00:01&lt;00:00, 26.6MB/s]
 83%|########2 | 21.9M/26.4M [00:01&lt;00:00, 24.3MB/s]
 93%|#########2| 24.5M/26.4M [00:01&lt;00:00, 24.5MB/s]
100%|##########| 26.4M/26.4M [00:01&lt;00:00, 17.8MB/s]

  0%|          | 0.00/29.5k [00:00&lt;?, ?B/s]
100%|##########| 29.5k/29.5k [00:00&lt;00:00, 325kB/s]

  0%|          | 0.00/4.42M [00:00&lt;?, ?B/s]
  1%|1         | 65.5k/4.42M [00:00&lt;00:12, 357kB/s]
  5%|5         | 229k/4.42M [00:00&lt;00:06, 671kB/s]
 21%|##        | 918k/4.42M [00:00&lt;00:01, 2.53MB/s]
 44%|####3     | 1.93M/4.42M [00:00&lt;00:00, 4.03MB/s]
100%|##########| 4.42M/4.42M [00:00&lt;00:00, 5.99MB/s]

  0%|          | 0.00/5.15k [00:00&lt;?, ?B/s]
100%|##########| 5.15k/5.15k [00:00&lt;00:00, 36.9MB/s]
</pre>
  </div>
 </div>
 <div class=""section"" id=""totensor"">
  <h2>
   ToTensor()
   <a class=""headerlink"" href=""#totensor"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   <a class=""reference external"" href=""https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ToTensor"">
    ToTensor
   </a>
   converts a PIL image or NumPy
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     ndarray
    </span>
   </code>
   into a
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     FloatTensor
    </span>
   </code>
   . and scales
the image’s pixel intensity values in the range [0., 1.]
  </p>
 </div>
 <div class=""section"" id=""lambda-transforms"">
  <h2>
   Lambda Transforms
   <a class=""headerlink"" href=""#lambda-transforms"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   Lambda transforms apply any user-defined lambda function. Here, we define a function
to turn the integer into a one-hot encoded tensor.
It first creates a zero tensor of size 10 (the number of labels in our dataset) and calls
   <a class=""reference external"" href=""https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html"">
    scatter_
   </a>
   which assigns a
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     value=1
    </span>
   </code>
   on the index as given by the label
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     y
    </span>
   </code>
   .
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><a class=""sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/vision/stable/generated/torchvision.transforms.Lambda.html#torchvision.transforms.Lambda"" title=""torchvision.transforms.Lambda""><span class=""n"">target_transform</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class"" href=""https://pytorch.org/vision/stable/generated/torchvision.transforms.Lambda.html#torchvision.transforms.Lambda"" title=""torchvision.transforms.Lambda""><span class=""n"">Lambda</span></a><span class=""p"">(</span><span class=""k"">lambda</span> <span class=""n"">y</span><span class=""p"">:</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros"" title=""torch.zeros""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">zeros</span></a><span class=""p"">(</span>
    <span class=""mi"">10</span><span class=""p"">,</span> <span class=""n"">dtype</span><span class=""o"">=</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype"" title=""torch.dtype""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">float</span></a><span class=""p"">)</span><span class=""o"">.</span><span class=""n"">scatter_</span><span class=""p"">(</span><span class=""n"">dim</span><span class=""o"">=</span><span class=""mi"">0</span><span class=""p"">,</span> <span class=""n"">index</span><span class=""o"">=</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"" title=""torch.tensor""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">tensor</span></a><span class=""p"">(</span><span class=""n"">y</span><span class=""p"">),</span> <span class=""n"">value</span><span class=""o"">=</span><span class=""mi"">1</span><span class=""p"">))</span>
</pre>
   </div>
  </div>
  <hr class=""docutils""/>
  <div class=""section"" id=""further-reading"">
   <h3>
    Further Reading
    <a class=""headerlink"" href=""#further-reading"" title=""Permalink to this heading"">
     ¶
    </a>
   </h3>
   <ul class=""simple"">
    <li>
     <p>
      <a class=""reference external"" href=""https://pytorch.org/vision/stable/transforms.html"">
       torchvision.transforms API
      </a>
     </p>
    </li>
   </ul>
   <p class=""sphx-glr-timing"">
    <strong>
     Total running time of the script:
    </strong>
    ( 0 minutes  4.475 seconds)
   </p>
   <div class=""sphx-glr-footer sphx-glr-footer-example docutils container"" id=""sphx-glr-download-beginner-basics-transforms-tutorial-py"">
    <div class=""sphx-glr-download sphx-glr-download-python docutils container"">
     <p>
      <a class=""reference download internal"" download="""" href=""../../_downloads/2f1ec3031a7101e25403c5d53a40a401/transforms_tutorial.py"">
       <code class=""xref download docutils literal notranslate"">
        <span class=""pre"">
         Download
        </span>
        <span class=""pre"">
         Python
        </span>
        <span class=""pre"">
         source
        </span>
        <span class=""pre"">
         code:
        </span>
        <span class=""pre"">
         transforms_tutorial.py
        </span>
       </code>
      </a>
     </p>
    </div>
    <div class=""sphx-glr-download sphx-glr-download-jupyter docutils container"">
     <p>
      <a class=""reference download internal"" download="""" href=""../../_downloads/9bdb71ef4a637dc36fb461904ccb7056/transforms_tutorial.ipynb"">
       <code class=""xref download docutils literal notranslate"">
        <span class=""pre"">
         Download
        </span>
        <span class=""pre"">
         Jupyter
        </span>
        <span class=""pre"">
         notebook:
        </span>
        <span class=""pre"">
         transforms_tutorial.ipynb
        </span>
       </code>
      </a>
     </p>
    </div>
   </div>
   <p class=""sphx-glr-signature"">
    <a class=""reference external"" href=""https://sphinx-gallery.github.io"">
     Gallery generated by Sphinx-Gallery
    </a>
   </p>
  </div>
 </div>
</div>
"
Build Model,https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html,"<div class=""section"" id=""build-the-neural-network"">
 <h1>
  Build the Neural Network
  <a class=""headerlink"" href=""#build-the-neural-network"" title=""Permalink to this heading"">
   ¶
  </a>
 </h1>
 <p class=""date-info-last-verified"" style=""color: #6c6c6d; font-size: small;"">
  Created On: Feb 09, 2021 | Last Updated: Jan 24, 2025 | Last Verified: Not Verified
 </p>
 <p>
  Neural networks comprise of layers/modules that perform operations on data.
The
  <a class=""reference external"" href=""https://pytorch.org/docs/stable/nn.html"">
   torch.nn
  </a>
  namespace provides all the building blocks you need to
build your own neural network. Every module in PyTorch subclasses the
  <a class=""reference external"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html"">
   nn.Module
  </a>
  .
A neural network is a module itself that consists of other modules (layers). This nested structure allows for
building and managing complex architectures easily.
 </p>
 <p>
  In the following sections, we’ll build a neural network to classify images in the FashionMNIST dataset.
 </p>
 <div class=""highlight-default notranslate"">
  <div class=""highlight"">
   <pre><span></span><span class=""kn"">import</span><span class=""w""> </span><span class=""nn"">os</span>
<span class=""kn"">import</span><span class=""w""> </span><span class=""nn"">torch</span>
<span class=""kn"">from</span><span class=""w""> </span><span class=""nn"">torch</span><span class=""w""> </span><span class=""kn"">import</span> <span class=""n"">nn</span>
<span class=""kn"">from</span><span class=""w""> </span><span class=""nn"">torch.utils.data</span><span class=""w""> </span><span class=""kn"">import</span> <span class=""n"">DataLoader</span>
<span class=""kn"">from</span><span class=""w""> </span><span class=""nn"">torchvision</span><span class=""w""> </span><span class=""kn"">import</span> <span class=""n"">datasets</span><span class=""p"">,</span> <span class=""n"">transforms</span>
</pre>
  </div>
 </div>
 <div class=""section"" id=""get-device-for-training"">
  <h2>
   Get Device for Training
   <a class=""headerlink"" href=""#get-device-for-training"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   We want to be able to train our model on an
   <a class=""reference external"" href=""https://pytorch.org/docs/stable/torch.html#accelerators"">
    accelerator
   </a>
   such as CUDA, MPS, MTIA, or XPU. If the current accelerator is available, we will use it. Otherwise, we use the CPU.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""n"">device</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-accelerator sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.accelerator.current_accelerator.html#torch.accelerator.current_accelerator"" title=""torch.accelerator.current_accelerator""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">accelerator</span><span class=""o"">.</span><span class=""n"">current_accelerator</span></a><span class=""p"">()</span><span class=""o"">.</span><span class=""n"">type</span> <span class=""k"">if</span> <a class=""sphx-glr-backref-module-torch-accelerator sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.accelerator.is_available.html#torch.accelerator.is_available"" title=""torch.accelerator.is_available""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">accelerator</span><span class=""o"">.</span><span class=""n"">is_available</span></a><span class=""p"">()</span> <span class=""k"">else</span> <span class=""s2"">""cpu""</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Using </span><span class=""si"">{</span><span class=""n"">device</span><span class=""si"">}</span><span class=""s2""> device""</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>Using cuda device
</pre>
   </div>
  </div>
 </div>
 <div class=""section"" id=""define-the-class"">
  <h2>
   Define the Class
   <a class=""headerlink"" href=""#define-the-class"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   We define our neural network by subclassing
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     nn.Module
    </span>
   </code>
   , and
initialize the neural network layers in
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     __init__
    </span>
   </code>
   . Every
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     nn.Module
    </span>
   </code>
   subclass implements
the operations on input data in the
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     forward
    </span>
   </code>
   method.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""k"">class</span><span class=""w""> </span><span class=""nc"">NeuralNetwork</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module"" title=""torch.nn.Module""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Module</span></a><span class=""p"">):</span>
    <span class=""k"">def</span><span class=""w""> </span><span class=""fm"">__init__</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">):</span>
        <span class=""nb"">super</span><span class=""p"">()</span><span class=""o"">.</span><span class=""fm"">__init__</span><span class=""p"">()</span>
        <span class=""bp"">self</span><span class=""o"">.</span><a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten"" title=""torch.nn.Flatten""><span class=""n"">flatten</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten"" title=""torch.nn.Flatten""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Flatten</span></a><span class=""p"">()</span>
        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">linear_relu_stack</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential"" title=""torch.nn.Sequential""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Sequential</span></a><span class=""p"">(</span>
            <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear"" title=""torch.nn.Linear""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Linear</span></a><span class=""p"">(</span><span class=""mi"">28</span><span class=""o"">*</span><span class=""mi"">28</span><span class=""p"">,</span> <span class=""mi"">512</span><span class=""p"">),</span>
            <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU"" title=""torch.nn.ReLU""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">ReLU</span></a><span class=""p"">(),</span>
            <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear"" title=""torch.nn.Linear""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Linear</span></a><span class=""p"">(</span><span class=""mi"">512</span><span class=""p"">,</span> <span class=""mi"">512</span><span class=""p"">),</span>
            <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU"" title=""torch.nn.ReLU""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">ReLU</span></a><span class=""p"">(),</span>
            <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear"" title=""torch.nn.Linear""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Linear</span></a><span class=""p"">(</span><span class=""mi"">512</span><span class=""p"">,</span> <span class=""mi"">10</span><span class=""p"">),</span>
        <span class=""p"">)</span>

    <span class=""k"">def</span><span class=""w""> </span><span class=""nf"">forward</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">,</span> <span class=""n"">x</span><span class=""p"">):</span>
        <span class=""n"">x</span> <span class=""o"">=</span> <span class=""bp"">self</span><span class=""o"">.</span><a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten"" title=""torch.nn.Flatten""><span class=""n"">flatten</span></a><span class=""p"">(</span><span class=""n"">x</span><span class=""p"">)</span>
        <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">logits</span></a> <span class=""o"">=</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">linear_relu_stack</span><span class=""p"">(</span><span class=""n"">x</span><span class=""p"">)</span>
        <span class=""k"">return</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">logits</span></a>
</pre>
   </div>
  </div>
  <p>
   We create an instance of
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     NeuralNetwork
    </span>
   </code>
   , and move it to the
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     device
    </span>
   </code>
   , and print
its structure.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""n"">model</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module"" title=""torch.nn.Module""><span class=""n"">NeuralNetwork</span></a><span class=""p"">()</span><span class=""o"">.</span><span class=""n"">to</span><span class=""p"">(</span><span class=""n"">device</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""n"">model</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>NeuralNetwork(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=784, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=10, bias=True)
  )
)
</pre>
   </div>
  </div>
  <p>
   To use the model, we pass it the input data. This executes the model’s
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     forward
    </span>
   </code>
   ,
along with some
   <a class=""reference external"" href=""https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866"">
    background operations
   </a>
   .
Do not call
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     model.forward()
    </span>
   </code>
   directly!
  </p>
  <p>
   Calling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of each output.
We get the prediction probabilities by passing it through an instance of the
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     nn.Softmax
    </span>
   </code>
   module.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">X</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand"" title=""torch.rand""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">rand</span></a><span class=""p"">(</span><span class=""mi"">1</span><span class=""p"">,</span> <span class=""mi"">28</span><span class=""p"">,</span> <span class=""mi"">28</span><span class=""p"">,</span> <span class=""n"">device</span><span class=""o"">=</span><span class=""n"">device</span><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">logits</span></a> <span class=""o"">=</span> <span class=""n"">model</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">X</span></a><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">pred_probab</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax"" title=""torch.nn.Softmax""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Softmax</span></a><span class=""p"">(</span><span class=""n"">dim</span><span class=""o"">=</span><span class=""mi"">1</span><span class=""p"">)(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">logits</span></a><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">y_pred</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">pred_probab</span></a><span class=""o"">.</span><span class=""n"">argmax</span><span class=""p"">(</span><span class=""mi"">1</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Predicted class: </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">y_pred</span></a><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>Predicted class: tensor([7], device='cuda:0')
</pre>
   </div>
  </div>
  <hr class=""docutils""/>
 </div>
 <div class=""section"" id=""model-layers"">
  <h2>
   Model Layers
   <a class=""headerlink"" href=""#model-layers"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   Let’s break down the layers in the FashionMNIST model. To illustrate it, we
will take a sample minibatch of 3 images of size 28x28 and see what happens to it as
we pass it through the network.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">input_image</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand"" title=""torch.rand""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">rand</span></a><span class=""p"">(</span><span class=""mi"">3</span><span class=""p"">,</span><span class=""mi"">28</span><span class=""p"">,</span><span class=""mi"">28</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">input_image</span></a><span class=""o"">.</span><span class=""n"">size</span><span class=""p"">())</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>torch.Size([3, 28, 28])
</pre>
   </div>
  </div>
  <div class=""section"" id=""nn-flatten"">
   <h3>
    nn.Flatten
    <a class=""headerlink"" href=""#nn-flatten"" title=""Permalink to this heading"">
     ¶
    </a>
   </h3>
   <p>
    We initialize the
    <a class=""reference external"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html"">
     nn.Flatten
    </a>
    layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values (
the minibatch dimension (at dim=0) is maintained).
   </p>
   <div class=""highlight-default notranslate"">
    <div class=""highlight"">
     <pre><span></span><a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten"" title=""torch.nn.Flatten""><span class=""n"">flatten</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten"" title=""torch.nn.Flatten""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Flatten</span></a><span class=""p"">()</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">flat_image</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten"" title=""torch.nn.Flatten""><span class=""n"">flatten</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">input_image</span></a><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">flat_image</span></a><span class=""o"">.</span><span class=""n"">size</span><span class=""p"">())</span>
</pre>
    </div>
   </div>
   <div class=""sphx-glr-script-out highlight-none notranslate"">
    <div class=""highlight"">
     <pre><span></span>torch.Size([3, 784])
</pre>
    </div>
   </div>
  </div>
  <div class=""section"" id=""nn-linear"">
   <h3>
    nn.Linear
    <a class=""headerlink"" href=""#nn-linear"" title=""Permalink to this heading"">
     ¶
    </a>
   </h3>
   <p>
    The
    <a class=""reference external"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"">
     linear layer
    </a>
    is a module that applies a linear transformation on the input using its stored weights and biases.
   </p>
   <div class=""highlight-default notranslate"">
    <div class=""highlight"">
     <pre><span></span><a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear"" title=""torch.nn.Linear""><span class=""n"">layer1</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear"" title=""torch.nn.Linear""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Linear</span></a><span class=""p"">(</span><span class=""n"">in_features</span><span class=""o"">=</span><span class=""mi"">28</span><span class=""o"">*</span><span class=""mi"">28</span><span class=""p"">,</span> <span class=""n"">out_features</span><span class=""o"">=</span><span class=""mi"">20</span><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">hidden1</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear"" title=""torch.nn.Linear""><span class=""n"">layer1</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">flat_image</span></a><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">hidden1</span></a><span class=""o"">.</span><span class=""n"">size</span><span class=""p"">())</span>
</pre>
    </div>
   </div>
   <div class=""sphx-glr-script-out highlight-none notranslate"">
    <div class=""highlight"">
     <pre><span></span>torch.Size([3, 20])
</pre>
    </div>
   </div>
  </div>
  <div class=""section"" id=""nn-relu"">
   <h3>
    nn.ReLU
    <a class=""headerlink"" href=""#nn-relu"" title=""Permalink to this heading"">
     ¶
    </a>
   </h3>
   <p>
    Non-linear activations are what create the complex mappings between the model’s inputs and outputs.
They are applied after linear transformations to introduce
    <em>
     nonlinearity
    </em>
    , helping neural networks
learn a wide variety of phenomena.
   </p>
   <p>
    In this model, we use
    <a class=""reference external"" href=""https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html"">
     nn.ReLU
    </a>
    between our
linear layers, but there’s other activations to introduce non-linearity in your model.
   </p>
   <div class=""highlight-default notranslate"">
    <div class=""highlight"">
     <pre><span></span><span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Before ReLU: </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">hidden1</span></a><span class=""si"">}</span><span class=""se"">\n\n</span><span class=""s2"">""</span><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">hidden1</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU"" title=""torch.nn.ReLU""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">ReLU</span></a><span class=""p"">()(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">hidden1</span></a><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""After ReLU: </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">hidden1</span></a><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
</pre>
    </div>
   </div>
   <div class=""sphx-glr-script-out highlight-none notranslate"">
    <div class=""highlight"">
     <pre><span></span>Before ReLU: tensor([[ 0.4158, -0.0130, -0.1144,  0.3960,  0.1476, -0.0690, -0.0269,  0.2690,
          0.1353,  0.1975,  0.4484,  0.0753,  0.4455,  0.5321, -0.1692,  0.4504,
          0.2476, -0.1787, -0.2754,  0.2462],
        [ 0.2326,  0.0623, -0.2984,  0.2878,  0.2767, -0.5434, -0.5051,  0.4339,
          0.0302,  0.1634,  0.5649, -0.0055,  0.2025,  0.4473, -0.2333,  0.6611,
          0.1883, -0.1250,  0.0820,  0.2778],
        [ 0.3325,  0.2654,  0.1091,  0.0651,  0.3425, -0.3880, -0.0152,  0.2298,
          0.3872,  0.0342,  0.8503,  0.0937,  0.1796,  0.5007, -0.1897,  0.4030,
          0.1189, -0.3237,  0.2048,  0.4343]], grad_fn=&lt;AddmmBackward0&gt;)


After ReLU: tensor([[0.4158, 0.0000, 0.0000, 0.3960, 0.1476, 0.0000, 0.0000, 0.2690, 0.1353,
         0.1975, 0.4484, 0.0753, 0.4455, 0.5321, 0.0000, 0.4504, 0.2476, 0.0000,
         0.0000, 0.2462],
        [0.2326, 0.0623, 0.0000, 0.2878, 0.2767, 0.0000, 0.0000, 0.4339, 0.0302,
         0.1634, 0.5649, 0.0000, 0.2025, 0.4473, 0.0000, 0.6611, 0.1883, 0.0000,
         0.0820, 0.2778],
        [0.3325, 0.2654, 0.1091, 0.0651, 0.3425, 0.0000, 0.0000, 0.2298, 0.3872,
         0.0342, 0.8503, 0.0937, 0.1796, 0.5007, 0.0000, 0.4030, 0.1189, 0.0000,
         0.2048, 0.4343]], grad_fn=&lt;ReluBackward0&gt;)
</pre>
    </div>
   </div>
  </div>
  <div class=""section"" id=""nn-sequential"">
   <h3>
    nn.Sequential
    <a class=""headerlink"" href=""#nn-sequential"" title=""Permalink to this heading"">
     ¶
    </a>
   </h3>
   <p>
    <a class=""reference external"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html"">
     nn.Sequential
    </a>
    is an ordered
container of modules. The data is passed through all the modules in the same order as defined. You can use
sequential containers to put together a quick network like
    <code class=""docutils literal notranslate"">
     <span class=""pre"">
      seq_modules
     </span>
    </code>
    .
   </p>
   <div class=""highlight-default notranslate"">
    <div class=""highlight"">
     <pre><span></span><a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential"" title=""torch.nn.Sequential""><span class=""n"">seq_modules</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential"" title=""torch.nn.Sequential""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Sequential</span></a><span class=""p"">(</span>
    <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten"" title=""torch.nn.Flatten""><span class=""n"">flatten</span></a><span class=""p"">,</span>
    <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear"" title=""torch.nn.Linear""><span class=""n"">layer1</span></a><span class=""p"">,</span>
    <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU"" title=""torch.nn.ReLU""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">ReLU</span></a><span class=""p"">(),</span>
    <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear"" title=""torch.nn.Linear""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Linear</span></a><span class=""p"">(</span><span class=""mi"">20</span><span class=""p"">,</span> <span class=""mi"">10</span><span class=""p"">)</span>
<span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">input_image</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand"" title=""torch.rand""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">rand</span></a><span class=""p"">(</span><span class=""mi"">3</span><span class=""p"">,</span><span class=""mi"">28</span><span class=""p"">,</span><span class=""mi"">28</span><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">logits</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential"" title=""torch.nn.Sequential""><span class=""n"">seq_modules</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">input_image</span></a><span class=""p"">)</span>
</pre>
    </div>
   </div>
  </div>
  <div class=""section"" id=""nn-softmax"">
   <h3>
    nn.Softmax
    <a class=""headerlink"" href=""#nn-softmax"" title=""Permalink to this heading"">
     ¶
    </a>
   </h3>
   <p>
    The last linear layer of the neural network returns
    <cite>
     logits
    </cite>
    - raw values in [-infty, infty] - which are passed to the
    <a class=""reference external"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html"">
     nn.Softmax
    </a>
    module. The logits are scaled to values
[0, 1] representing the model’s predicted probabilities for each class.
    <code class=""docutils literal notranslate"">
     <span class=""pre"">
      dim
     </span>
    </code>
    parameter indicates the dimension along
which the values must sum to 1.
   </p>
   <div class=""highlight-default notranslate"">
    <div class=""highlight"">
     <pre><span></span><a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax"" title=""torch.nn.Softmax""><span class=""n"">softmax</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax"" title=""torch.nn.Softmax""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Softmax</span></a><span class=""p"">(</span><span class=""n"">dim</span><span class=""o"">=</span><span class=""mi"">1</span><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">pred_probab</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#torch.nn.Softmax"" title=""torch.nn.Softmax""><span class=""n"">softmax</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">logits</span></a><span class=""p"">)</span>
</pre>
    </div>
   </div>
  </div>
 </div>
 <div class=""section"" id=""model-parameters"">
  <h2>
   Model Parameters
   <a class=""headerlink"" href=""#model-parameters"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   Many layers inside a neural network are
   <em>
    parameterized
   </em>
   , i.e. have associated weights
and biases that are optimized during training. Subclassing
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     nn.Module
    </span>
   </code>
   automatically
tracks all fields defined inside your model object, and makes all parameters
accessible using your model’s
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     parameters()
    </span>
   </code>
   or
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     named_parameters()
    </span>
   </code>
   methods.
  </p>
  <p>
   In this example, we iterate over each parameter, and print its size and a preview of its values.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Model structure: </span><span class=""si"">{</span><span class=""n"">model</span><span class=""si"">}</span><span class=""se"">\n\n</span><span class=""s2"">""</span><span class=""p"">)</span>

<span class=""k"">for</span> <span class=""n"">name</span><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch-nn-parameter sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter"" title=""torch.nn.parameter.Parameter""><span class=""n"">param</span></a> <span class=""ow"">in</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.named_parameters"" title=""torch.nn.Module.named_parameters""><span class=""n"">model</span><span class=""o"">.</span><span class=""n"">named_parameters</span></a><span class=""p"">():</span>
    <span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Layer: </span><span class=""si"">{</span><span class=""n"">name</span><span class=""si"">}</span><span class=""s2""> | Size: </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch-nn-parameter sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter"" title=""torch.nn.parameter.Parameter""><span class=""n"">param</span></a><span class=""o"">.</span><span class=""n"">size</span><span class=""p"">()</span><span class=""si"">}</span><span class=""s2""> | Values : </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch-nn-parameter sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter"" title=""torch.nn.parameter.Parameter""><span class=""n"">param</span></a><span class=""p"">[:</span><span class=""mi"">2</span><span class=""p"">]</span><span class=""si"">}</span><span class=""s2""> </span><span class=""se"">\n</span><span class=""s2"">""</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>Model structure: NeuralNetwork(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=784, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=512, bias=True)
    (3): ReLU()
    (4): Linear(in_features=512, out_features=10, bias=True)
  )
)


Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0273,  0.0296, -0.0084,  ..., -0.0142,  0.0093,  0.0135],
        [-0.0188, -0.0354,  0.0187,  ..., -0.0106, -0.0001,  0.0115]],
       device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)

Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0155, -0.0327], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)

Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[ 0.0116,  0.0293, -0.0280,  ...,  0.0334, -0.0078,  0.0298],
        [ 0.0095,  0.0038,  0.0009,  ..., -0.0365, -0.0011, -0.0221]],
       device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)

Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([ 0.0148, -0.0256], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)

Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0147, -0.0229,  0.0180,  ..., -0.0013,  0.0177,  0.0070],
        [-0.0202, -0.0417, -0.0279,  ..., -0.0441,  0.0185, -0.0268]],
       device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)

Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0070, -0.0411], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)
</pre>
   </div>
  </div>
  <hr class=""docutils""/>
 </div>
 <div class=""section"" id=""further-reading"">
  <h2>
   Further Reading
   <a class=""headerlink"" href=""#further-reading"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <ul class=""simple"">
   <li>
    <p>
     <a class=""reference external"" href=""https://pytorch.org/docs/stable/nn.html"">
      torch.nn API
     </a>
    </p>
   </li>
  </ul>
  <p class=""sphx-glr-timing"">
   <strong>
    Total running time of the script:
   </strong>
   ( 0 minutes  0.139 seconds)
  </p>
  <div class=""sphx-glr-footer sphx-glr-footer-example docutils container"" id=""sphx-glr-download-beginner-basics-buildmodel-tutorial-py"">
   <div class=""sphx-glr-download sphx-glr-download-python docutils container"">
    <p>
     <a class=""reference download internal"" download="""" href=""../../_downloads/ac800c8c4c9c372154788058b1e89246/buildmodel_tutorial.py"">
      <code class=""xref download docutils literal notranslate"">
       <span class=""pre"">
        Download
       </span>
       <span class=""pre"">
        Python
       </span>
       <span class=""pre"">
        source
       </span>
       <span class=""pre"">
        code:
       </span>
       <span class=""pre"">
        buildmodel_tutorial.py
       </span>
      </code>
     </a>
    </p>
   </div>
   <div class=""sphx-glr-download sphx-glr-download-jupyter docutils container"">
    <p>
     <a class=""reference download internal"" download="""" href=""../../_downloads/76d764ad694d0795e494a1edbfb068a6/buildmodel_tutorial.ipynb"">
      <code class=""xref download docutils literal notranslate"">
       <span class=""pre"">
        Download
       </span>
       <span class=""pre"">
        Jupyter
       </span>
       <span class=""pre"">
        notebook:
       </span>
       <span class=""pre"">
        buildmodel_tutorial.ipynb
       </span>
      </code>
     </a>
    </p>
   </div>
  </div>
  <p class=""sphx-glr-signature"">
   <a class=""reference external"" href=""https://sphinx-gallery.github.io"">
    Gallery generated by Sphinx-Gallery
   </a>
  </p>
 </div>
</div>
"
Automatic Differentiation,https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html,"<div class=""section"" id=""automatic-differentiation-with-torch-autograd"">
 <h1>
  Automatic Differentiation with
  <code class=""docutils literal notranslate"">
   <span class=""pre"">
    torch.autograd
   </span>
  </code>
  <a class=""headerlink"" href=""#automatic-differentiation-with-torch-autograd"" title=""Permalink to this heading"">
   ¶
  </a>
 </h1>
 <p class=""date-info-last-verified"" style=""color: #6c6c6d; font-size: small;"">
  Created On: Feb 10, 2021 | Last Updated: Jan 16, 2024 | Last Verified: Nov 05, 2024
 </p>
 <p>
  When training neural networks, the most frequently used algorithm is
  <strong>
   back propagation
  </strong>
  . In this algorithm, parameters (model weights) are
adjusted according to the
  <strong>
   gradient
  </strong>
  of the loss function with respect
to the given parameter.
 </p>
 <p>
  To compute those gradients, PyTorch has a built-in differentiation engine
called
  <code class=""docutils literal notranslate"">
   <span class=""pre"">
    torch.autograd
   </span>
  </code>
  . It supports automatic computation of gradient for any
computational graph.
 </p>
 <p>
  Consider the simplest one-layer neural network, with input
  <code class=""docutils literal notranslate"">
   <span class=""pre"">
    x
   </span>
  </code>
  ,
parameters
  <code class=""docutils literal notranslate"">
   <span class=""pre"">
    w
   </span>
  </code>
  and
  <code class=""docutils literal notranslate"">
   <span class=""pre"">
    b
   </span>
  </code>
  , and some loss function. It can be defined in
PyTorch in the following manner:
 </p>
 <div class=""highlight-default notranslate"">
  <div class=""highlight"">
   <pre><span></span><span class=""kn"">import</span><span class=""w""> </span><span class=""nn"">torch</span>

<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">x</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"" title=""torch.ones""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">ones</span></a><span class=""p"">(</span><span class=""mi"">5</span><span class=""p"">)</span>  <span class=""c1""># input tensor</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">y</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros"" title=""torch.zeros""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">zeros</span></a><span class=""p"">(</span><span class=""mi"">3</span><span class=""p"">)</span>  <span class=""c1""># expected output</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">w</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn"" title=""torch.randn""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">randn</span></a><span class=""p"">(</span><span class=""mi"">5</span><span class=""p"">,</span> <span class=""mi"">3</span><span class=""p"">,</span> <span class=""n"">requires_grad</span><span class=""o"">=</span><span class=""kc"">True</span><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">b</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn"" title=""torch.randn""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">randn</span></a><span class=""p"">(</span><span class=""mi"">3</span><span class=""p"">,</span> <span class=""n"">requires_grad</span><span class=""o"">=</span><span class=""kc"">True</span><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">z</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"" title=""torch.matmul""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">matmul</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">x</span></a><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">w</span></a><span class=""p"">)</span><span class=""o"">+</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">b</span></a>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">loss</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn-functional sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy_with_logits.html#torch.nn.functional.binary_cross_entropy_with_logits"" title=""torch.nn.functional.binary_cross_entropy_with_logits""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">functional</span><span class=""o"">.</span><span class=""n"">binary_cross_entropy_with_logits</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">z</span></a><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">y</span></a><span class=""p"">)</span>
</pre>
  </div>
 </div>
 <div class=""section"" id=""tensors-functions-and-computational-graph"">
  <h2>
   Tensors, Functions and Computational graph
   <a class=""headerlink"" href=""#tensors-functions-and-computational-graph"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   This code defines the following
   <strong>
    computational graph
   </strong>
   :
  </p>
  <div class=""figure align-default"">
   <img alt="""" src=""https://pytorch.org/tutorials/_images/comp-graph.png""/>
  </div>
  <p>
   In this network,
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     w
    </span>
   </code>
   and
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     b
    </span>
   </code>
   are
   <strong>
    parameters
   </strong>
   , which we need to
optimize. Thus, we need to be able to compute the gradients of loss
function with respect to those variables. In order to do that, we set
the
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     requires_grad
    </span>
   </code>
   property of those tensors.
  </p>
  <div class=""admonition note"">
   <p class=""admonition-title"">
    Note
   </p>
   <p>
    You can set the value of
    <code class=""docutils literal notranslate"">
     <span class=""pre"">
      requires_grad
     </span>
    </code>
    when creating a
tensor, or later by using
    <code class=""docutils literal notranslate"">
     <span class=""pre"">
      x.requires_grad_(True)
     </span>
    </code>
    method.
   </p>
  </div>
  <p>
   A function that we apply to tensors to construct computational graph is
in fact an object of class
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     Function
    </span>
   </code>
   . This object knows how to
compute the function in the
   <em>
    forward
   </em>
   direction, and also how to compute
its derivative during the
   <em>
    backward propagation
   </em>
   step. A reference to
the backward propagation function is stored in
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     grad_fn
    </span>
   </code>
   property of a
tensor. You can find more information of
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     Function
    </span>
   </code>
   <a class=""reference external"" href=""https://pytorch.org/docs/stable/autograd.html#function"">
    in the
documentation
   </a>
   .
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Gradient function for z = </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">z</span></a><span class=""o"">.</span><span class=""n"">grad_fn</span><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Gradient function for loss = </span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">loss</span></a><span class=""o"">.</span><span class=""n"">grad_fn</span><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>Gradient function for z = &lt;AddBackward0 object at 0x7fd0c07d18d0&gt;
Gradient function for loss = &lt;BinaryCrossEntropyWithLogitsBackward0 object at 0x7fd0c07d0ee0&gt;
</pre>
   </div>
  </div>
 </div>
 <div class=""section"" id=""computing-gradients"">
  <h2>
   Computing Gradients
   <a class=""headerlink"" href=""#computing-gradients"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   To optimize weights of parameters in the neural network, we need to
compute the derivatives of our loss function with respect to parameters,
namely, we need
   <span class=""math"">
    \(\frac{\partial loss}{\partial w}\)
   </span>
   and
   <span class=""math"">
    \(\frac{\partial loss}{\partial b}\)
   </span>
   under some fixed values of
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     x
    </span>
   </code>
   and
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     y
    </span>
   </code>
   . To compute those derivatives, we call
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     loss.backward()
    </span>
   </code>
   , and then retrieve the values from
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     w.grad
    </span>
   </code>
   and
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     b.grad
    </span>
   </code>
   :
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-method"" href=""https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward"" title=""torch.Tensor.backward""><span class=""n"">loss</span><span class=""o"">.</span><span class=""n"">backward</span></a><span class=""p"">()</span>
<span class=""nb"">print</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">w</span><span class=""o"">.</span><span class=""n"">grad</span></a><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">b</span><span class=""o"">.</span><span class=""n"">grad</span></a><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>tensor([[0.3313, 0.0626, 0.2530],
        [0.3313, 0.0626, 0.2530],
        [0.3313, 0.0626, 0.2530],
        [0.3313, 0.0626, 0.2530],
        [0.3313, 0.0626, 0.2530]])
tensor([0.3313, 0.0626, 0.2530])
</pre>
   </div>
  </div>
  <div class=""admonition note"">
   <p class=""admonition-title"">
    Note
   </p>
   <ul class=""simple"">
    <li>
     <p>
      We can only obtain the
      <code class=""docutils literal notranslate"">
       <span class=""pre"">
        grad
       </span>
      </code>
      properties for the leaf
nodes of the computational graph, which have
      <code class=""docutils literal notranslate"">
       <span class=""pre"">
        requires_grad
       </span>
      </code>
      property
set to
      <code class=""docutils literal notranslate"">
       <span class=""pre"">
        True
       </span>
      </code>
      . For all other nodes in our graph, gradients will not be
available.
     </p>
    </li>
    <li>
     <p>
      We can only perform gradient calculations using
      <code class=""docutils literal notranslate"">
       <span class=""pre"">
        backward
       </span>
      </code>
      once on a given graph, for performance reasons. If we need
to do several
      <code class=""docutils literal notranslate"">
       <span class=""pre"">
        backward
       </span>
      </code>
      calls on the same graph, we need to pass
      <code class=""docutils literal notranslate"">
       <span class=""pre"">
        retain_graph=True
       </span>
      </code>
      to the
      <code class=""docutils literal notranslate"">
       <span class=""pre"">
        backward
       </span>
      </code>
      call.
     </p>
    </li>
   </ul>
  </div>
 </div>
 <div class=""section"" id=""disabling-gradient-tracking"">
  <h2>
   Disabling Gradient Tracking
   <a class=""headerlink"" href=""#disabling-gradient-tracking"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   By default, all tensors with
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     requires_grad=True
    </span>
   </code>
   are tracking their
computational history and support gradient computation. However, there
are some cases when we do not need to do that, for example, when we have
trained the model and just want to apply it to some input data, i.e. we
only want to do
   <em>
    forward
   </em>
   computations through the network. We can stop
tracking computations by surrounding our computation code with
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     torch.no_grad()
    </span>
   </code>
   block:
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">z</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"" title=""torch.matmul""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">matmul</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">x</span></a><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">w</span></a><span class=""p"">)</span><span class=""o"">+</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">b</span></a>
<span class=""nb"">print</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">z</span></a><span class=""o"">.</span><span class=""n"">requires_grad</span><span class=""p"">)</span>

<span class=""k"">with</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad"" title=""torch.no_grad""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">no_grad</span></a><span class=""p"">():</span>
    <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">z</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"" title=""torch.matmul""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">matmul</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">x</span></a><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">w</span></a><span class=""p"">)</span><span class=""o"">+</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">b</span></a>
<span class=""nb"">print</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">z</span></a><span class=""o"">.</span><span class=""n"">requires_grad</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>True
False
</pre>
   </div>
  </div>
  <p>
   Another way to achieve the same result is to use the
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     detach()
    </span>
   </code>
   method
on the tensor:
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">z</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"" title=""torch.matmul""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">matmul</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">x</span></a><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">w</span></a><span class=""p"">)</span><span class=""o"">+</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">b</span></a>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">z_det</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">z</span></a><span class=""o"">.</span><span class=""n"">detach</span><span class=""p"">()</span>
<span class=""nb"">print</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">z_det</span></a><span class=""o"">.</span><span class=""n"">requires_grad</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>False
</pre>
   </div>
  </div>
  <dl class=""simple"">
   <dt>
    There are reasons you might want to disable gradient tracking:
   </dt>
   <dd>
    <ul class=""simple"">
     <li>
      <p>
       To mark some parameters in your neural network as
       <strong>
        frozen parameters
       </strong>
       .
      </p>
     </li>
     <li>
      <p>
       To
       <strong>
        speed up computations
       </strong>
       when you are only doing forward pass, because computations on tensors that do
not track gradients would be more efficient.
      </p>
     </li>
    </ul>
   </dd>
  </dl>
 </div>
 <div class=""section"" id=""more-on-computational-graphs"">
  <h2>
   More on Computational Graphs
   <a class=""headerlink"" href=""#more-on-computational-graphs"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   Conceptually, autograd keeps a record of data (tensors) and all executed
operations (along with the resulting new tensors) in a directed acyclic
graph (DAG) consisting of
   <a class=""reference external"" href=""https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function"">
    Function
   </a>
   objects. In this DAG, leaves are the input tensors, roots are the output
tensors. By tracing this graph from roots to leaves, you can
automatically compute the gradients using the chain rule.
  </p>
  <p>
   In a forward pass, autograd does two things simultaneously:
  </p>
  <ul class=""simple"">
   <li>
    <p>
     run the requested operation to compute a resulting tensor
    </p>
   </li>
   <li>
    <p>
     maintain the operation’s
     <em>
      gradient function
     </em>
     in the DAG.
    </p>
   </li>
  </ul>
  <p>
   The backward pass kicks off when
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     .backward()
    </span>
   </code>
   is called on the DAG
root.
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     autograd
    </span>
   </code>
   then:
  </p>
  <ul class=""simple"">
   <li>
    <p>
     computes the gradients from each
     <code class=""docutils literal notranslate"">
      <span class=""pre"">
       .grad_fn
      </span>
     </code>
     ,
    </p>
   </li>
   <li>
    <p>
     accumulates them in the respective tensor’s
     <code class=""docutils literal notranslate"">
      <span class=""pre"">
       .grad
      </span>
     </code>
     attribute
    </p>
   </li>
   <li>
    <p>
     using the chain rule, propagates all the way to the leaf tensors.
    </p>
   </li>
  </ul>
  <div class=""admonition note"">
   <p class=""admonition-title"">
    Note
   </p>
   <p>
    <strong>
     DAGs are dynamic in PyTorch
    </strong>
    An important thing to note is that the graph is recreated from scratch; after each
    <code class=""docutils literal notranslate"">
     <span class=""pre"">
      .backward()
     </span>
    </code>
    call, autograd starts populating a new graph. This is
exactly what allows you to use control flow statements in your model;
you can change the shape, size and operations at every iteration if
needed.
   </p>
  </div>
 </div>
 <div class=""section"" id=""optional-reading-tensor-gradients-and-jacobian-products"">
  <h2>
   Optional Reading: Tensor Gradients and Jacobian Products
   <a class=""headerlink"" href=""#optional-reading-tensor-gradients-and-jacobian-products"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   In many cases, we have a scalar loss function, and we need to compute
the gradient with respect to some parameters. However, there are cases
when the output function is an arbitrary tensor. In this case, PyTorch
allows you to compute so-called
   <strong>
    Jacobian product
   </strong>
   , and not the actual
gradient.
  </p>
  <p>
   For a vector function
   <span class=""math"">
    \(\vec{y}=f(\vec{x})\)
   </span>
   , where
   <span class=""math"">
    \(\vec{x}=\langle x_1,\dots,x_n\rangle\)
   </span>
   and
   <span class=""math"">
    \(\vec{y}=\langle y_1,\dots,y_m\rangle\)
   </span>
   , a gradient of
   <span class=""math"">
    \(\vec{y}\)
   </span>
   with respect to
   <span class=""math"">
    \(\vec{x}\)
   </span>
   is given by
   <strong>
    Jacobian
matrix
   </strong>
   :
  </p>
  <div class=""math"">
   \[J=\left(\begin{array}{ccc}
   \frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{1}}{\partial x_{n}}\\
   \vdots &amp; \ddots &amp; \vdots\\
   \frac{\partial y_{m}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}}
   \end{array}\right)\]
  </div>
  <p>
   Instead of computing the Jacobian matrix itself, PyTorch allows you to
compute
   <strong>
    Jacobian Product
   </strong>
   <span class=""math"">
    \(v^T\cdot J\)
   </span>
   for a given input vector
   <span class=""math"">
    \(v=(v_1 \dots v_m)\)
   </span>
   . This is achieved by calling
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     backward
    </span>
   </code>
   with
   <span class=""math"">
    \(v\)
   </span>
   as an argument. The size of
   <span class=""math"">
    \(v\)
   </span>
   should be the same as
the size of the original tensor, with respect to which we want to
compute the product:
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">inp</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"" title=""torch.eye""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">eye</span></a><span class=""p"">(</span><span class=""mi"">4</span><span class=""p"">,</span> <span class=""mi"">5</span><span class=""p"">,</span> <span class=""n"">requires_grad</span><span class=""o"">=</span><span class=""kc"">True</span><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">out</span></a> <span class=""o"">=</span> <span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">inp</span></a><span class=""o"">+</span><span class=""mi"">1</span><span class=""p"">)</span><span class=""o"">.</span><span class=""n"">pow</span><span class=""p"">(</span><span class=""mi"">2</span><span class=""p"">)</span><span class=""o"">.</span><span class=""n"">t</span><span class=""p"">()</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-method"" href=""https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward"" title=""torch.Tensor.backward""><span class=""n"">out</span><span class=""o"">.</span><span class=""n"">backward</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.ones_like.html#torch.ones_like"" title=""torch.ones_like""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">ones_like</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">out</span></a><span class=""p"">),</span> <span class=""n"">retain_graph</span><span class=""o"">=</span><span class=""kc"">True</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""First call</span><span class=""se"">\n</span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">inp</span><span class=""o"">.</span><span class=""n"">grad</span></a><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-method"" href=""https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward"" title=""torch.Tensor.backward""><span class=""n"">out</span><span class=""o"">.</span><span class=""n"">backward</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.ones_like.html#torch.ones_like"" title=""torch.ones_like""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">ones_like</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">out</span></a><span class=""p"">),</span> <span class=""n"">retain_graph</span><span class=""o"">=</span><span class=""kc"">True</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""</span><span class=""se"">\n</span><span class=""s2"">Second call</span><span class=""se"">\n</span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">inp</span><span class=""o"">.</span><span class=""n"">grad</span></a><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">inp</span><span class=""o"">.</span><span class=""n"">grad</span></a><span class=""o"">.</span><span class=""n"">zero_</span><span class=""p"">()</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-method"" href=""https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward"" title=""torch.Tensor.backward""><span class=""n"">out</span><span class=""o"">.</span><span class=""n"">backward</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.ones_like.html#torch.ones_like"" title=""torch.ones_like""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">ones_like</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">out</span></a><span class=""p"">),</span> <span class=""n"">retain_graph</span><span class=""o"">=</span><span class=""kc"">True</span><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""</span><span class=""se"">\n</span><span class=""s2"">Call after zeroing gradients</span><span class=""se"">\n</span><span class=""si"">{</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor"" title=""torch.Tensor""><span class=""n"">inp</span><span class=""o"">.</span><span class=""n"">grad</span></a><span class=""si"">}</span><span class=""s2"">""</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>First call
tensor([[4., 2., 2., 2., 2.],
        [2., 4., 2., 2., 2.],
        [2., 2., 4., 2., 2.],
        [2., 2., 2., 4., 2.]])

Second call
tensor([[8., 4., 4., 4., 4.],
        [4., 8., 4., 4., 4.],
        [4., 4., 8., 4., 4.],
        [4., 4., 4., 8., 4.]])

Call after zeroing gradients
tensor([[4., 2., 2., 2., 2.],
        [2., 4., 2., 2., 2.],
        [2., 2., 4., 2., 2.],
        [2., 2., 2., 4., 2.]])
</pre>
   </div>
  </div>
  <p>
   Notice that when we call
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     backward
    </span>
   </code>
   for the second time with the same
argument, the value of the gradient is different. This happens because
when doing
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     backward
    </span>
   </code>
   propagation, PyTorch
   <strong>
    accumulates the
gradients
   </strong>
   , i.e. the value of computed gradients is added to the
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     grad
    </span>
   </code>
   property of all leaf nodes of computational graph. If you want
to compute the proper gradients, you need to zero out the
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     grad
    </span>
   </code>
   property before. In real-life training an
   <em>
    optimizer
   </em>
   helps us to do
this.
  </p>
  <div class=""admonition note"">
   <p class=""admonition-title"">
    Note
   </p>
   <p>
    Previously we were calling
    <code class=""docutils literal notranslate"">
     <span class=""pre"">
      backward()
     </span>
    </code>
    function without
parameters. This is essentially equivalent to calling
    <code class=""docutils literal notranslate"">
     <span class=""pre"">
      backward(torch.tensor(1.0))
     </span>
    </code>
    , which is a useful way to compute the
gradients in case of a scalar-valued function, such as loss during
neural network training.
   </p>
  </div>
  <hr class=""docutils""/>
  <div class=""section"" id=""further-reading"">
   <h3>
    Further Reading
    <a class=""headerlink"" href=""#further-reading"" title=""Permalink to this heading"">
     ¶
    </a>
   </h3>
   <ul class=""simple"">
    <li>
     <p>
      <a class=""reference external"" href=""https://pytorch.org/docs/stable/notes/autograd.html"">
       Autograd Mechanics
      </a>
     </p>
    </li>
   </ul>
   <p class=""sphx-glr-timing"">
    <strong>
     Total running time of the script:
    </strong>
    ( 0 minutes  0.013 seconds)
   </p>
   <div class=""sphx-glr-footer sphx-glr-footer-example docutils container"" id=""sphx-glr-download-beginner-basics-autogradqs-tutorial-py"">
    <div class=""sphx-glr-download sphx-glr-download-python docutils container"">
     <p>
      <a class=""reference download internal"" download="""" href=""../../_downloads/fbf83d81ea8e82d633984f21bab274cc/autogradqs_tutorial.py"">
       <code class=""xref download docutils literal notranslate"">
        <span class=""pre"">
         Download
        </span>
        <span class=""pre"">
         Python
        </span>
        <span class=""pre"">
         source
        </span>
        <span class=""pre"">
         code:
        </span>
        <span class=""pre"">
         autogradqs_tutorial.py
        </span>
       </code>
      </a>
     </p>
    </div>
    <div class=""sphx-glr-download sphx-glr-download-jupyter docutils container"">
     <p>
      <a class=""reference download internal"" download="""" href=""../../_downloads/ad7e62b138c384adac98888ce94ff659/autogradqs_tutorial.ipynb"">
       <code class=""xref download docutils literal notranslate"">
        <span class=""pre"">
         Download
        </span>
        <span class=""pre"">
         Jupyter
        </span>
        <span class=""pre"">
         notebook:
        </span>
        <span class=""pre"">
         autogradqs_tutorial.ipynb
        </span>
       </code>
      </a>
     </p>
    </div>
   </div>
   <p class=""sphx-glr-signature"">
    <a class=""reference external"" href=""https://sphinx-gallery.github.io"">
     Gallery generated by Sphinx-Gallery
    </a>
   </p>
  </div>
 </div>
</div>
"
Optimization Loop,https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html,"<div class=""section"" id=""optimizing-model-parameters"">
 <h1>
  Optimizing Model Parameters
  <a class=""headerlink"" href=""#optimizing-model-parameters"" title=""Permalink to this heading"">
   ¶
  </a>
 </h1>
 <p class=""date-info-last-verified"" style=""color: #6c6c6d; font-size: small;"">
  Created On: Feb 09, 2021 | Last Updated: Jan 31, 2024 | Last Verified: Nov 05, 2024
 </p>
 <p>
  Now that we have a model and data it’s time to train, validate and test our model by optimizing its parameters on
our data. Training a model is an iterative process; in each iteration the model makes a guess about the output, calculates
the error in its guess (
  <em>
   loss
  </em>
  ), collects the derivatives of the error with respect to its parameters (as we saw in
the
  <a class=""reference external"" href=""autograd_tutorial.html"">
   previous section
  </a>
  ), and
  <strong>
   optimizes
  </strong>
  these parameters using gradient descent. For a more
detailed walkthrough of this process, check out this video on
  <a class=""reference external"" href=""https://www.youtube.com/watch?v=tIeHLnjs5U8"">
   backpropagation from 3Blue1Brown
  </a>
  .
 </p>
 <div class=""section"" id=""prerequisite-code"">
  <h2>
   Prerequisite Code
   <a class=""headerlink"" href=""#prerequisite-code"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   We load the code from the previous sections on
   <a class=""reference external"" href=""data_tutorial.html"">
    Datasets &amp; DataLoaders
   </a>
   and
   <a class=""reference external"" href=""buildmodel_tutorial.html"">
    Build Model
   </a>
   .
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""kn"">import</span><span class=""w""> </span><span class=""nn"">torch</span>
<span class=""kn"">from</span><span class=""w""> </span><span class=""nn"">torch</span><span class=""w""> </span><span class=""kn"">import</span> <span class=""n"">nn</span>
<span class=""kn"">from</span><span class=""w""> </span><span class=""nn"">torch.utils.data</span><span class=""w""> </span><span class=""kn"">import</span> <a class=""sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" title=""torch.utils.data.DataLoader""><span class=""n"">DataLoader</span></a>
<span class=""kn"">from</span><span class=""w""> </span><span class=""nn"">torchvision</span><span class=""w""> </span><span class=""kn"">import</span> <span class=""n"">datasets</span>
<span class=""kn"">from</span><span class=""w""> </span><span class=""nn"">torchvision.transforms</span><span class=""w""> </span><span class=""kn"">import</span> <a class=""sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class"" href=""https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor"" title=""torchvision.transforms.ToTensor""><span class=""n"">ToTensor</span></a>

<a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">training_data</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">datasets</span><span class=""o"">.</span><span class=""n"">FashionMNIST</span></a><span class=""p"">(</span>
    <span class=""n"">root</span><span class=""o"">=</span><span class=""s2"">""data""</span><span class=""p"">,</span>
    <span class=""n"">train</span><span class=""o"">=</span><span class=""kc"">True</span><span class=""p"">,</span>
    <span class=""n"">download</span><span class=""o"">=</span><span class=""kc"">True</span><span class=""p"">,</span>
    <span class=""n"">transform</span><span class=""o"">=</span><a class=""sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class"" href=""https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor"" title=""torchvision.transforms.ToTensor""><span class=""n"">ToTensor</span></a><span class=""p"">()</span>
<span class=""p"">)</span>

<a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">test_data</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">datasets</span><span class=""o"">.</span><span class=""n"">FashionMNIST</span></a><span class=""p"">(</span>
    <span class=""n"">root</span><span class=""o"">=</span><span class=""s2"">""data""</span><span class=""p"">,</span>
    <span class=""n"">train</span><span class=""o"">=</span><span class=""kc"">False</span><span class=""p"">,</span>
    <span class=""n"">download</span><span class=""o"">=</span><span class=""kc"">True</span><span class=""p"">,</span>
    <span class=""n"">transform</span><span class=""o"">=</span><a class=""sphx-glr-backref-module-torchvision-transforms sphx-glr-backref-type-py-class"" href=""https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html#torchvision.transforms.ToTensor"" title=""torchvision.transforms.ToTensor""><span class=""n"">ToTensor</span></a><span class=""p"">()</span>
<span class=""p"">)</span>

<a class=""sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" title=""torch.utils.data.DataLoader""><span class=""n"">train_dataloader</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" title=""torch.utils.data.DataLoader""><span class=""n"">DataLoader</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">training_data</span></a><span class=""p"">,</span> <span class=""n"">batch_size</span><span class=""o"">=</span><span class=""mi"">64</span><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" title=""torch.utils.data.DataLoader""><span class=""n"">test_dataloader</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" title=""torch.utils.data.DataLoader""><span class=""n"">DataLoader</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torchvision-datasets sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST"" title=""torchvision.datasets.FashionMNIST""><span class=""n"">test_data</span></a><span class=""p"">,</span> <span class=""n"">batch_size</span><span class=""o"">=</span><span class=""mi"">64</span><span class=""p"">)</span>

<span class=""k"">class</span><span class=""w""> </span><span class=""nc"">NeuralNetwork</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module"" title=""torch.nn.Module""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Module</span></a><span class=""p"">):</span>
    <span class=""k"">def</span><span class=""w""> </span><span class=""fm"">__init__</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">):</span>
        <span class=""nb"">super</span><span class=""p"">()</span><span class=""o"">.</span><span class=""fm"">__init__</span><span class=""p"">()</span>
        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">flatten</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten"" title=""torch.nn.Flatten""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Flatten</span></a><span class=""p"">()</span>
        <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">linear_relu_stack</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential"" title=""torch.nn.Sequential""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Sequential</span></a><span class=""p"">(</span>
            <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear"" title=""torch.nn.Linear""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Linear</span></a><span class=""p"">(</span><span class=""mi"">28</span><span class=""o"">*</span><span class=""mi"">28</span><span class=""p"">,</span> <span class=""mi"">512</span><span class=""p"">),</span>
            <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU"" title=""torch.nn.ReLU""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">ReLU</span></a><span class=""p"">(),</span>
            <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear"" title=""torch.nn.Linear""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Linear</span></a><span class=""p"">(</span><span class=""mi"">512</span><span class=""p"">,</span> <span class=""mi"">512</span><span class=""p"">),</span>
            <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU"" title=""torch.nn.ReLU""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">ReLU</span></a><span class=""p"">(),</span>
            <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear"" title=""torch.nn.Linear""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">Linear</span></a><span class=""p"">(</span><span class=""mi"">512</span><span class=""p"">,</span> <span class=""mi"">10</span><span class=""p"">),</span>
        <span class=""p"">)</span>

    <span class=""k"">def</span><span class=""w""> </span><span class=""nf"">forward</span><span class=""p"">(</span><span class=""bp"">self</span><span class=""p"">,</span> <span class=""n"">x</span><span class=""p"">):</span>
        <span class=""n"">x</span> <span class=""o"">=</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">flatten</span><span class=""p"">(</span><span class=""n"">x</span><span class=""p"">)</span>
        <span class=""n"">logits</span> <span class=""o"">=</span> <span class=""bp"">self</span><span class=""o"">.</span><span class=""n"">linear_relu_stack</span><span class=""p"">(</span><span class=""n"">x</span><span class=""p"">)</span>
        <span class=""k"">return</span> <span class=""n"">logits</span>

<span class=""n"">model</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module"" title=""torch.nn.Module""><span class=""n"">NeuralNetwork</span></a><span class=""p"">()</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>  0%|          | 0.00/26.4M [00:00&lt;?, ?B/s]
  0%|          | 65.5k/26.4M [00:00&lt;01:13, 361kB/s]
  1%|          | 229k/26.4M [00:00&lt;00:38, 676kB/s]
  3%|3         | 885k/26.4M [00:00&lt;00:12, 2.04MB/s]
 10%|9         | 2.56M/26.4M [00:00&lt;00:04, 5.95MB/s]
 25%|##5       | 6.62M/26.4M [00:00&lt;00:01, 12.9MB/s]
 39%|###9      | 10.4M/26.4M [00:00&lt;00:00, 19.0MB/s]
 60%|#####9    | 15.8M/26.4M [00:01&lt;00:00, 23.7MB/s]
 71%|#######1  | 18.8M/26.4M [00:01&lt;00:00, 25.1MB/s]
 94%|#########3| 24.8M/26.4M [00:01&lt;00:00, 28.1MB/s]
100%|##########| 26.4M/26.4M [00:01&lt;00:00, 19.2MB/s]

  0%|          | 0.00/29.5k [00:00&lt;?, ?B/s]
100%|##########| 29.5k/29.5k [00:00&lt;00:00, 326kB/s]

  0%|          | 0.00/4.42M [00:00&lt;?, ?B/s]
  1%|1         | 65.5k/4.42M [00:00&lt;00:12, 360kB/s]
  5%|5         | 229k/4.42M [00:00&lt;00:06, 680kB/s]
 20%|##        | 885k/4.42M [00:00&lt;00:01, 2.53MB/s]
 44%|####3     | 1.93M/4.42M [00:00&lt;00:00, 4.10MB/s]
100%|##########| 4.42M/4.42M [00:00&lt;00:00, 6.08MB/s]

  0%|          | 0.00/5.15k [00:00&lt;?, ?B/s]
100%|##########| 5.15k/5.15k [00:00&lt;00:00, 34.4MB/s]
</pre>
   </div>
  </div>
 </div>
 <div class=""section"" id=""hyperparameters"">
  <h2>
   Hyperparameters
   <a class=""headerlink"" href=""#hyperparameters"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   Hyperparameters are adjustable parameters that let you control the model optimization process.
Different hyperparameter values can impact model training and convergence rates
(
   <a class=""reference external"" href=""https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html"">
    read more
   </a>
   about hyperparameter tuning)
  </p>
  <dl class=""simple"">
   <dt>
    We define the following hyperparameters for training:
   </dt>
   <dd>
    <ul class=""simple"">
     <li>
      <p>
       <strong>
        Number of Epochs
       </strong>
       - the number times to iterate over the dataset
      </p>
     </li>
     <li>
      <p>
       <strong>
        Batch Size
       </strong>
       - the number of data samples propagated through the network before the parameters are updated
      </p>
     </li>
     <li>
      <p>
       <strong>
        Learning Rate
       </strong>
       - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training.
      </p>
     </li>
    </ul>
   </dd>
  </dl>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""n"">learning_rate</span> <span class=""o"">=</span> <span class=""mf"">1e-3</span>
<span class=""n"">batch_size</span> <span class=""o"">=</span> <span class=""mi"">64</span>
<span class=""n"">epochs</span> <span class=""o"">=</span> <span class=""mi"">5</span>
</pre>
   </div>
  </div>
 </div>
 <div class=""section"" id=""optimization-loop"">
  <h2>
   Optimization Loop
   <a class=""headerlink"" href=""#optimization-loop"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   Once we set our hyperparameters, we can then train and optimize our model with an optimization loop. Each
iteration of the optimization loop is called an
   <strong>
    epoch
   </strong>
   .
  </p>
  <dl class=""simple"">
   <dt>
    Each epoch consists of two main parts:
   </dt>
   <dd>
    <ul class=""simple"">
     <li>
      <p>
       <strong>
        The Train Loop
       </strong>
       - iterate over the training dataset and try to converge to optimal parameters.
      </p>
     </li>
     <li>
      <p>
       <strong>
        The Validation/Test Loop
       </strong>
       - iterate over the test dataset to check if model performance is improving.
      </p>
     </li>
    </ul>
   </dd>
  </dl>
  <p>
   Let’s briefly familiarize ourselves with some of the concepts used in the training loop. Jump ahead to
see the
   <a class=""reference internal"" href=""#full-impl-label"">
    <span class=""std std-ref"">
     Full Implementation
    </span>
   </a>
   of the optimization loop.
  </p>
  <div class=""section"" id=""loss-function"">
   <h3>
    Loss Function
    <a class=""headerlink"" href=""#loss-function"" title=""Permalink to this heading"">
     ¶
    </a>
   </h3>
   <p>
    When presented with some training data, our untrained network is likely not to give the correct
answer.
    <strong>
     Loss function
    </strong>
    measures the degree of dissimilarity of obtained result to the target value,
and it is the loss function that we want to minimize during training. To calculate the loss we make a
prediction using the inputs of our given data sample and compare it against the true data label value.
   </p>
   <p>
    Common loss functions include
    <a class=""reference external"" href=""https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss"">
     nn.MSELoss
    </a>
    (Mean Square Error) for regression tasks, and
    <a class=""reference external"" href=""https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss"">
     nn.NLLLoss
    </a>
    (Negative Log Likelihood) for classification.
    <a class=""reference external"" href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"">
     nn.CrossEntropyLoss
    </a>
    combines
    <code class=""docutils literal notranslate"">
     <span class=""pre"">
      nn.LogSoftmax
     </span>
    </code>
    and
    <code class=""docutils literal notranslate"">
     <span class=""pre"">
      nn.NLLLoss
     </span>
    </code>
    .
   </p>
   <p>
    We pass our model’s output logits to
    <code class=""docutils literal notranslate"">
     <span class=""pre"">
      nn.CrossEntropyLoss
     </span>
    </code>
    , which will normalize the logits and compute the prediction error.
   </p>
   <div class=""highlight-default notranslate"">
    <div class=""highlight"">
     <pre><span></span><span class=""c1""># Initialize the loss function</span>
<a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"" title=""torch.nn.CrossEntropyLoss""><span class=""n"">loss_fn</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"" title=""torch.nn.CrossEntropyLoss""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">CrossEntropyLoss</span></a><span class=""p"">()</span>
</pre>
    </div>
   </div>
  </div>
  <div class=""section"" id=""optimizer"">
   <h3>
    Optimizer
    <a class=""headerlink"" href=""#optimizer"" title=""Permalink to this heading"">
     ¶
    </a>
   </h3>
   <p>
    Optimization is the process of adjusting model parameters to reduce model error in each training step.
    <strong>
     Optimization algorithms
    </strong>
    define how this process is performed (in this example we use Stochastic Gradient Descent).
All optimization logic is encapsulated in  the
    <code class=""docutils literal notranslate"">
     <span class=""pre"">
      optimizer
     </span>
    </code>
    object. Here, we use the SGD optimizer; additionally, there are many
    <a class=""reference external"" href=""https://pytorch.org/docs/stable/optim.html"">
     different optimizers
    </a>
    available in PyTorch such as ADAM and RMSProp, that work better for different kinds of models and data.
   </p>
   <p>
    We initialize the optimizer by registering the model’s parameters that need to be trained, and passing in the learning rate hyperparameter.
   </p>
   <div class=""highlight-default notranslate"">
    <div class=""highlight"">
     <pre><span></span><a class=""sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD"" title=""torch.optim.SGD""><span class=""n"">optimizer</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD"" title=""torch.optim.SGD""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">optim</span><span class=""o"">.</span><span class=""n"">SGD</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters"" title=""torch.nn.Module.parameters""><span class=""n"">model</span><span class=""o"">.</span><span class=""n"">parameters</span></a><span class=""p"">(),</span> <span class=""n"">lr</span><span class=""o"">=</span><span class=""n"">learning_rate</span><span class=""p"">)</span>
</pre>
    </div>
   </div>
   <dl class=""simple"">
    <dt>
     Inside the training loop, optimization happens in three steps:
    </dt>
    <dd>
     <ul class=""simple"">
      <li>
       <p>
        Call
        <code class=""docutils literal notranslate"">
         <span class=""pre"">
          optimizer.zero_grad()
         </span>
        </code>
        to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.
       </p>
      </li>
      <li>
       <p>
        Backpropagate the prediction loss with a call to
        <code class=""docutils literal notranslate"">
         <span class=""pre"">
          loss.backward()
         </span>
        </code>
        . PyTorch deposits the gradients of the loss w.r.t. each parameter.
       </p>
      </li>
      <li>
       <p>
        Once we have our gradients, we call
        <code class=""docutils literal notranslate"">
         <span class=""pre"">
          optimizer.step()
         </span>
        </code>
        to adjust the parameters by the gradients collected in the backward pass.
       </p>
      </li>
     </ul>
    </dd>
   </dl>
  </div>
 </div>
 <div class=""section"" id=""full-implementation"">
  <span id=""full-impl-label"">
  </span>
  <h2>
   Full Implementation
   <a class=""headerlink"" href=""#full-implementation"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   We define
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     train_loop
    </span>
   </code>
   that loops over our optimization code, and
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     test_loop
    </span>
   </code>
   that
evaluates the model’s performance against our test data.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""k"">def</span><span class=""w""> </span><span class=""nf"">train_loop</span><span class=""p"">(</span><span class=""n"">dataloader</span><span class=""p"">,</span> <span class=""n"">model</span><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"" title=""torch.nn.CrossEntropyLoss""><span class=""n"">loss_fn</span></a><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD"" title=""torch.optim.SGD""><span class=""n"">optimizer</span></a><span class=""p"">):</span>
    <span class=""n"">size</span> <span class=""o"">=</span> <span class=""nb"">len</span><span class=""p"">(</span><span class=""n"">dataloader</span><span class=""o"">.</span><span class=""n"">dataset</span><span class=""p"">)</span>
    <span class=""c1""># Set the model to training mode - important for batch normalization and dropout layers</span>
    <span class=""c1""># Unnecessary in this situation but added for best practices</span>
    <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train"" title=""torch.nn.Module.train""><span class=""n"">model</span><span class=""o"">.</span><span class=""n"">train</span></a><span class=""p"">()</span>
    <span class=""k"">for</span> <span class=""n"">batch</span><span class=""p"">,</span> <span class=""p"">(</span><span class=""n"">X</span><span class=""p"">,</span> <span class=""n"">y</span><span class=""p"">)</span> <span class=""ow"">in</span> <span class=""nb"">enumerate</span><span class=""p"">(</span><span class=""n"">dataloader</span><span class=""p"">):</span>
        <span class=""c1""># Compute prediction and loss</span>
        <span class=""n"">pred</span> <span class=""o"">=</span> <span class=""n"">model</span><span class=""p"">(</span><span class=""n"">X</span><span class=""p"">)</span>
        <span class=""n"">loss</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"" title=""torch.nn.CrossEntropyLoss""><span class=""n"">loss_fn</span></a><span class=""p"">(</span><span class=""n"">pred</span><span class=""p"">,</span> <span class=""n"">y</span><span class=""p"">)</span>

        <span class=""c1""># Backpropagation</span>
        <span class=""n"">loss</span><span class=""o"">.</span><span class=""n"">backward</span><span class=""p"">()</span>
        <a class=""sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-method"" href=""https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.step"" title=""torch.optim.SGD.step""><span class=""n"">optimizer</span><span class=""o"">.</span><span class=""n"">step</span></a><span class=""p"">()</span>
        <a class=""sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-method"" href=""https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD.zero_grad"" title=""torch.optim.SGD.zero_grad""><span class=""n"">optimizer</span><span class=""o"">.</span><span class=""n"">zero_grad</span></a><span class=""p"">()</span>

        <span class=""k"">if</span> <span class=""n"">batch</span> <span class=""o"">%</span> <span class=""mi"">100</span> <span class=""o"">==</span> <span class=""mi"">0</span><span class=""p"">:</span>
            <span class=""n"">loss</span><span class=""p"">,</span> <span class=""n"">current</span> <span class=""o"">=</span> <span class=""n"">loss</span><span class=""o"">.</span><span class=""n"">item</span><span class=""p"">(),</span> <span class=""n"">batch</span> <span class=""o"">*</span> <span class=""n"">batch_size</span> <span class=""o"">+</span> <span class=""nb"">len</span><span class=""p"">(</span><span class=""n"">X</span><span class=""p"">)</span>
            <span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""loss: </span><span class=""si"">{</span><span class=""n"">loss</span><span class=""si"">:</span><span class=""s2"">&gt;7f</span><span class=""si"">}</span><span class=""s2"">  [</span><span class=""si"">{</span><span class=""n"">current</span><span class=""si"">:</span><span class=""s2"">&gt;5d</span><span class=""si"">}</span><span class=""s2"">/</span><span class=""si"">{</span><span class=""n"">size</span><span class=""si"">:</span><span class=""s2"">&gt;5d</span><span class=""si"">}</span><span class=""s2"">]""</span><span class=""p"">)</span>


<span class=""k"">def</span><span class=""w""> </span><span class=""nf"">test_loop</span><span class=""p"">(</span><span class=""n"">dataloader</span><span class=""p"">,</span> <span class=""n"">model</span><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"" title=""torch.nn.CrossEntropyLoss""><span class=""n"">loss_fn</span></a><span class=""p"">):</span>
    <span class=""c1""># Set the model to evaluation mode - important for batch normalization and dropout layers</span>
    <span class=""c1""># Unnecessary in this situation but added for best practices</span>
    <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval"" title=""torch.nn.Module.eval""><span class=""n"">model</span><span class=""o"">.</span><span class=""n"">eval</span></a><span class=""p"">()</span>
    <span class=""n"">size</span> <span class=""o"">=</span> <span class=""nb"">len</span><span class=""p"">(</span><span class=""n"">dataloader</span><span class=""o"">.</span><span class=""n"">dataset</span><span class=""p"">)</span>
    <span class=""n"">num_batches</span> <span class=""o"">=</span> <span class=""nb"">len</span><span class=""p"">(</span><span class=""n"">dataloader</span><span class=""p"">)</span>
    <span class=""n"">test_loss</span><span class=""p"">,</span> <span class=""n"">correct</span> <span class=""o"">=</span> <span class=""mi"">0</span><span class=""p"">,</span> <span class=""mi"">0</span>

    <span class=""c1""># Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode</span>
    <span class=""c1""># also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True</span>
    <span class=""k"">with</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad"" title=""torch.no_grad""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">no_grad</span></a><span class=""p"">():</span>
        <span class=""k"">for</span> <span class=""n"">X</span><span class=""p"">,</span> <span class=""n"">y</span> <span class=""ow"">in</span> <span class=""n"">dataloader</span><span class=""p"">:</span>
            <span class=""n"">pred</span> <span class=""o"">=</span> <span class=""n"">model</span><span class=""p"">(</span><span class=""n"">X</span><span class=""p"">)</span>
            <span class=""n"">test_loss</span> <span class=""o"">+=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"" title=""torch.nn.CrossEntropyLoss""><span class=""n"">loss_fn</span></a><span class=""p"">(</span><span class=""n"">pred</span><span class=""p"">,</span> <span class=""n"">y</span><span class=""p"">)</span><span class=""o"">.</span><span class=""n"">item</span><span class=""p"">()</span>
            <span class=""n"">correct</span> <span class=""o"">+=</span> <span class=""p"">(</span><span class=""n"">pred</span><span class=""o"">.</span><span class=""n"">argmax</span><span class=""p"">(</span><span class=""mi"">1</span><span class=""p"">)</span> <span class=""o"">==</span> <span class=""n"">y</span><span class=""p"">)</span><span class=""o"">.</span><span class=""n"">type</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype"" title=""torch.dtype""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">float</span></a><span class=""p"">)</span><span class=""o"">.</span><span class=""n"">sum</span><span class=""p"">()</span><span class=""o"">.</span><span class=""n"">item</span><span class=""p"">()</span>

    <span class=""n"">test_loss</span> <span class=""o"">/=</span> <span class=""n"">num_batches</span>
    <span class=""n"">correct</span> <span class=""o"">/=</span> <span class=""n"">size</span>
    <span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Test Error: </span><span class=""se"">\n</span><span class=""s2""> Accuracy: </span><span class=""si"">{</span><span class=""p"">(</span><span class=""mi"">100</span><span class=""o"">*</span><span class=""n"">correct</span><span class=""p"">)</span><span class=""si"">:</span><span class=""s2"">&gt;0.1f</span><span class=""si"">}</span><span class=""s2"">%, Avg loss: </span><span class=""si"">{</span><span class=""n"">test_loss</span><span class=""si"">:</span><span class=""s2"">&gt;8f</span><span class=""si"">}</span><span class=""s2""> </span><span class=""se"">\n</span><span class=""s2"">""</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <p>
   We initialize the loss function and optimizer, and pass it to
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     train_loop
    </span>
   </code>
   and
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     test_loop
    </span>
   </code>
   .
Feel free to increase the number of epochs to track the model’s improving performance.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"" title=""torch.nn.CrossEntropyLoss""><span class=""n"">loss_fn</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"" title=""torch.nn.CrossEntropyLoss""><span class=""n"">nn</span><span class=""o"">.</span><span class=""n"">CrossEntropyLoss</span></a><span class=""p"">()</span>
<a class=""sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD"" title=""torch.optim.SGD""><span class=""n"">optimizer</span></a> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class"" href=""https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD"" title=""torch.optim.SGD""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">optim</span><span class=""o"">.</span><span class=""n"">SGD</span></a><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-method"" href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters"" title=""torch.nn.Module.parameters""><span class=""n"">model</span><span class=""o"">.</span><span class=""n"">parameters</span></a><span class=""p"">(),</span> <span class=""n"">lr</span><span class=""o"">=</span><span class=""n"">learning_rate</span><span class=""p"">)</span>

<span class=""n"">epochs</span> <span class=""o"">=</span> <span class=""mi"">10</span>
<span class=""k"">for</span> <span class=""n"">t</span> <span class=""ow"">in</span> <span class=""nb"">range</span><span class=""p"">(</span><span class=""n"">epochs</span><span class=""p"">):</span>
    <span class=""nb"">print</span><span class=""p"">(</span><span class=""sa"">f</span><span class=""s2"">""Epoch </span><span class=""si"">{</span><span class=""n"">t</span><span class=""o"">+</span><span class=""mi"">1</span><span class=""si"">}</span><span class=""se"">\n</span><span class=""s2"">-------------------------------""</span><span class=""p"">)</span>
    <span class=""n"">train_loop</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" title=""torch.utils.data.DataLoader""><span class=""n"">train_dataloader</span></a><span class=""p"">,</span> <span class=""n"">model</span><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"" title=""torch.nn.CrossEntropyLoss""><span class=""n"">loss_fn</span></a><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch-optim sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD"" title=""torch.optim.SGD""><span class=""n"">optimizer</span></a><span class=""p"">)</span>
    <span class=""n"">test_loop</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch-utils-data sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"" title=""torch.utils.data.DataLoader""><span class=""n"">test_dataloader</span></a><span class=""p"">,</span> <span class=""n"">model</span><span class=""p"">,</span> <a class=""sphx-glr-backref-module-torch-nn sphx-glr-backref-type-py-class sphx-glr-backref-instance"" href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss"" title=""torch.nn.CrossEntropyLoss""><span class=""n"">loss_fn</span></a><span class=""p"">)</span>
<span class=""nb"">print</span><span class=""p"">(</span><span class=""s2"">""Done!""</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>Epoch 1
-------------------------------
loss: 2.298730  [   64/60000]
loss: 2.289123  [ 6464/60000]
loss: 2.273286  [12864/60000]
loss: 2.269406  [19264/60000]
loss: 2.249603  [25664/60000]
loss: 2.229407  [32064/60000]
loss: 2.227368  [38464/60000]
loss: 2.204261  [44864/60000]
loss: 2.206193  [51264/60000]
loss: 2.166651  [57664/60000]
Test Error:
 Accuracy: 50.9%, Avg loss: 2.166725

Epoch 2
-------------------------------
loss: 2.176750  [   64/60000]
loss: 2.169595  [ 6464/60000]
loss: 2.117500  [12864/60000]
loss: 2.129272  [19264/60000]
loss: 2.079674  [25664/60000]
loss: 2.032928  [32064/60000]
loss: 2.050115  [38464/60000]
loss: 1.985236  [44864/60000]
loss: 1.987887  [51264/60000]
loss: 1.907162  [57664/60000]
Test Error:
 Accuracy: 55.9%, Avg loss: 1.915486

Epoch 3
-------------------------------
loss: 1.951612  [   64/60000]
loss: 1.928685  [ 6464/60000]
loss: 1.815709  [12864/60000]
loss: 1.841552  [19264/60000]
loss: 1.732467  [25664/60000]
loss: 1.692914  [32064/60000]
loss: 1.701714  [38464/60000]
loss: 1.610632  [44864/60000]
loss: 1.632870  [51264/60000]
loss: 1.514263  [57664/60000]
Test Error:
 Accuracy: 58.8%, Avg loss: 1.541525

Epoch 4
-------------------------------
loss: 1.616448  [   64/60000]
loss: 1.582892  [ 6464/60000]
loss: 1.427595  [12864/60000]
loss: 1.487950  [19264/60000]
loss: 1.359332  [25664/60000]
loss: 1.364817  [32064/60000]
loss: 1.371491  [38464/60000]
loss: 1.298706  [44864/60000]
loss: 1.336201  [51264/60000]
loss: 1.232145  [57664/60000]
Test Error:
 Accuracy: 62.2%, Avg loss: 1.260237

Epoch 5
-------------------------------
loss: 1.345538  [   64/60000]
loss: 1.327798  [ 6464/60000]
loss: 1.153802  [12864/60000]
loss: 1.254829  [19264/60000]
loss: 1.117322  [25664/60000]
loss: 1.153248  [32064/60000]
loss: 1.171765  [38464/60000]
loss: 1.110263  [44864/60000]
loss: 1.154467  [51264/60000]
loss: 1.070921  [57664/60000]
Test Error:
 Accuracy: 64.1%, Avg loss: 1.089831

Epoch 6
-------------------------------
loss: 1.166889  [   64/60000]
loss: 1.170514  [ 6464/60000]
loss: 0.979435  [12864/60000]
loss: 1.113774  [19264/60000]
loss: 0.973411  [25664/60000]
loss: 1.015192  [32064/60000]
loss: 1.051113  [38464/60000]
loss: 0.993591  [44864/60000]
loss: 1.039709  [51264/60000]
loss: 0.971077  [57664/60000]
Test Error:
 Accuracy: 65.8%, Avg loss: 0.982440

Epoch 7
-------------------------------
loss: 1.045165  [   64/60000]
loss: 1.070583  [ 6464/60000]
loss: 0.862304  [12864/60000]
loss: 1.022265  [19264/60000]
loss: 0.885213  [25664/60000]
loss: 0.919528  [32064/60000]
loss: 0.972762  [38464/60000]
loss: 0.918728  [44864/60000]
loss: 0.961629  [51264/60000]
loss: 0.904379  [57664/60000]
Test Error:
 Accuracy: 66.9%, Avg loss: 0.910167

Epoch 8
-------------------------------
loss: 0.956964  [   64/60000]
loss: 1.002171  [ 6464/60000]
loss: 0.779057  [12864/60000]
loss: 0.958409  [19264/60000]
loss: 0.827240  [25664/60000]
loss: 0.850262  [32064/60000]
loss: 0.917320  [38464/60000]
loss: 0.868384  [44864/60000]
loss: 0.905506  [51264/60000]
loss: 0.856353  [57664/60000]
Test Error:
 Accuracy: 68.3%, Avg loss: 0.858248

Epoch 9
-------------------------------
loss: 0.889765  [   64/60000]
loss: 0.951220  [ 6464/60000]
loss: 0.717035  [12864/60000]
loss: 0.911042  [19264/60000]
loss: 0.786085  [25664/60000]
loss: 0.798370  [32064/60000]
loss: 0.874939  [38464/60000]
loss: 0.832796  [44864/60000]
loss: 0.863254  [51264/60000]
loss: 0.819742  [57664/60000]
Test Error:
 Accuracy: 69.5%, Avg loss: 0.818780

Epoch 10
-------------------------------
loss: 0.836395  [   64/60000]
loss: 0.910220  [ 6464/60000]
loss: 0.668506  [12864/60000]
loss: 0.874338  [19264/60000]
loss: 0.754805  [25664/60000]
loss: 0.758453  [32064/60000]
loss: 0.840451  [38464/60000]
loss: 0.806153  [44864/60000]
loss: 0.830360  [51264/60000]
loss: 0.790281  [57664/60000]
Test Error:
 Accuracy: 71.0%, Avg loss: 0.787271

Done!
</pre>
   </div>
  </div>
 </div>
 <div class=""section"" id=""further-reading"">
  <h2>
   Further Reading
   <a class=""headerlink"" href=""#further-reading"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <ul class=""simple"">
   <li>
    <p>
     <a class=""reference external"" href=""https://pytorch.org/docs/stable/nn.html#loss-functions"">
      Loss Functions
     </a>
    </p>
   </li>
   <li>
    <p>
     <a class=""reference external"" href=""https://pytorch.org/docs/stable/optim.html"">
      torch.optim
     </a>
    </p>
   </li>
   <li>
    <p>
     <a class=""reference external"" href=""https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html"">
      Warmstart Training a Model
     </a>
    </p>
   </li>
  </ul>
  <p class=""sphx-glr-timing"">
   <strong>
    Total running time of the script:
   </strong>
   ( 2 minutes  1.813 seconds)
  </p>
  <div class=""sphx-glr-footer sphx-glr-footer-example docutils container"" id=""sphx-glr-download-beginner-basics-optimization-tutorial-py"">
   <div class=""sphx-glr-download sphx-glr-download-python docutils container"">
    <p>
     <a class=""reference download internal"" download="""" href=""../../_downloads/0662a149d54bd776924742c96eb6282d/optimization_tutorial.py"">
      <code class=""xref download docutils literal notranslate"">
       <span class=""pre"">
        Download
       </span>
       <span class=""pre"">
        Python
       </span>
       <span class=""pre"">
        source
       </span>
       <span class=""pre"">
        code:
       </span>
       <span class=""pre"">
        optimization_tutorial.py
       </span>
      </code>
     </a>
    </p>
   </div>
   <div class=""sphx-glr-download sphx-glr-download-jupyter docutils container"">
    <p>
     <a class=""reference download internal"" download="""" href=""../../_downloads/91d72708edab956d7293bb263e2ab53f/optimization_tutorial.ipynb"">
      <code class=""xref download docutils literal notranslate"">
       <span class=""pre"">
        Download
       </span>
       <span class=""pre"">
        Jupyter
       </span>
       <span class=""pre"">
        notebook:
       </span>
       <span class=""pre"">
        optimization_tutorial.ipynb
       </span>
      </code>
     </a>
    </p>
   </div>
  </div>
  <p class=""sphx-glr-signature"">
   <a class=""reference external"" href=""https://sphinx-gallery.github.io"">
    Gallery generated by Sphinx-Gallery
   </a>
  </p>
 </div>
</div>
"
"Save, Load and Use Model",https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html,"<div class=""section"" id=""save-and-load-the-model"">
 <h1>
  Save and Load the Model
  <a class=""headerlink"" href=""#save-and-load-the-model"" title=""Permalink to this heading"">
   ¶
  </a>
 </h1>
 <p class=""date-info-last-verified"" style=""color: #6c6c6d; font-size: small;"">
  Created On: Feb 09, 2021 | Last Updated: Oct 15, 2024 | Last Verified: Nov 05, 2024
 </p>
 <p>
  In this section we will look at how to persist model state with saving, loading and running model predictions.
 </p>
 <div class=""highlight-default notranslate"">
  <div class=""highlight"">
   <pre><span></span><span class=""kn"">import</span><span class=""w""> </span><span class=""nn"">torch</span>
<span class=""kn"">import</span><span class=""w""> </span><span class=""nn"">torchvision.models</span><span class=""w""> </span><span class=""k"">as</span><span class=""w""> </span><span class=""nn"">models</span>
</pre>
  </div>
 </div>
 <div class=""section"" id=""saving-and-loading-model-weights"">
  <h2>
   Saving and Loading Model Weights
   <a class=""headerlink"" href=""#saving-and-loading-model-weights"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   PyTorch models store the learned parameters in an internal
state dictionary, called
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     state_dict
    </span>
   </code>
   . These can be persisted via the
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     torch.save
    </span>
   </code>
   method:
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""n"">model</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torchvision-models sphx-glr-backref-type-py-function"" href=""https://pytorch.org/vision/stable/models/generated/torchvision.models.vgg16.html#torchvision.models.vgg16"" title=""torchvision.models.vgg16""><span class=""n"">models</span><span class=""o"">.</span><span class=""n"">vgg16</span></a><span class=""p"">(</span><span class=""n"">weights</span><span class=""o"">=</span><span class=""s1"">'IMAGENET1K_V1'</span><span class=""p"">)</span>
<a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"" title=""torch.save""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">save</span></a><span class=""p"">(</span><span class=""n"">model</span><span class=""o"">.</span><span class=""n"">state_dict</span><span class=""p"">(),</span> <span class=""s1"">'model_weights.pth'</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>Downloading: ""https://download.pytorch.org/models/vgg16-397923af.pth"" to /var/lib/ci-user/.cache/torch/hub/checkpoints/vgg16-397923af.pth

  0%|          | 0.00/528M [00:00&lt;?, ?B/s]
  4%|3         | 20.6M/528M [00:00&lt;00:02, 216MB/s]
  8%|7         | 41.9M/528M [00:00&lt;00:02, 219MB/s]
 12%|#1        | 63.1M/528M [00:00&lt;00:02, 221MB/s]
 16%|#5        | 84.4M/528M [00:00&lt;00:02, 222MB/s]
 20%|##        | 106M/528M [00:00&lt;00:01, 222MB/s]
 24%|##4       | 127M/528M [00:00&lt;00:01, 222MB/s]
 28%|##8       | 148M/528M [00:00&lt;00:01, 223MB/s]
 32%|###2      | 170M/528M [00:00&lt;00:01, 223MB/s]
 36%|###6      | 191M/528M [00:00&lt;00:01, 223MB/s]
 40%|####      | 212M/528M [00:01&lt;00:01, 223MB/s]
 44%|####4     | 234M/528M [00:01&lt;00:01, 223MB/s]
 48%|####8     | 255M/528M [00:01&lt;00:01, 223MB/s]
 52%|#####2    | 276M/528M [00:01&lt;00:01, 223MB/s]
 56%|#####6    | 298M/528M [00:01&lt;00:01, 223MB/s]
 60%|######    | 319M/528M [00:01&lt;00:00, 223MB/s]
 65%|######4   | 341M/528M [00:01&lt;00:00, 223MB/s]
 69%|######8   | 362M/528M [00:01&lt;00:00, 223MB/s]
 73%|#######2  | 383M/528M [00:01&lt;00:00, 223MB/s]
 77%|#######6  | 405M/528M [00:01&lt;00:00, 223MB/s]
 81%|########  | 426M/528M [00:02&lt;00:00, 223MB/s]
 85%|########4 | 448M/528M [00:02&lt;00:00, 223MB/s]
 89%|########8 | 469M/528M [00:02&lt;00:00, 223MB/s]
 93%|#########2| 490M/528M [00:02&lt;00:00, 223MB/s]
 97%|#########6| 511M/528M [00:02&lt;00:00, 223MB/s]
100%|##########| 528M/528M [00:02&lt;00:00, 223MB/s]
</pre>
   </div>
  </div>
  <p>
   To load model weights, you need to create an instance of the same model first, and then load the parameters
using
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     load_state_dict()
    </span>
   </code>
   method.
  </p>
  <p>
   In the code below, we set
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     weights_only=True
    </span>
   </code>
   to limit the
functions executed during unpickling to only those necessary for
loading weights. Using
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     weights_only=True
    </span>
   </code>
   is considered
a best practice when loading weights.
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""n"">model</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torchvision-models sphx-glr-backref-type-py-function"" href=""https://pytorch.org/vision/stable/models/generated/torchvision.models.vgg16.html#torchvision.models.vgg16"" title=""torchvision.models.vgg16""><span class=""n"">models</span><span class=""o"">.</span><span class=""n"">vgg16</span></a><span class=""p"">()</span> <span class=""c1""># we do not specify ``weights``, i.e. create untrained model</span>
<span class=""n"">model</span><span class=""o"">.</span><span class=""n"">load_state_dict</span><span class=""p"">(</span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"" title=""torch.load""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">load</span></a><span class=""p"">(</span><span class=""s1"">'model_weights.pth'</span><span class=""p"">,</span> <span class=""n"">weights_only</span><span class=""o"">=</span><span class=""kc"">True</span><span class=""p"">))</span>
<span class=""n"">model</span><span class=""o"">.</span><span class=""n"">eval</span><span class=""p"">()</span>
</pre>
   </div>
  </div>
  <div class=""sphx-glr-script-out highlight-none notranslate"">
   <div class=""highlight"">
    <pre><span></span>VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace=True)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace=True)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
</pre>
   </div>
  </div>
  <div class=""admonition note"">
   <p class=""admonition-title"">
    Note
   </p>
   <p>
    be sure to call
    <code class=""docutils literal notranslate"">
     <span class=""pre"">
      model.eval()
     </span>
    </code>
    method before inferencing to set the dropout and batch normalization layers to evaluation mode. Failing to do this will yield inconsistent inference results.
   </p>
  </div>
 </div>
 <div class=""section"" id=""saving-and-loading-models-with-shapes"">
  <h2>
   Saving and Loading Models with Shapes
   <a class=""headerlink"" href=""#saving-and-loading-models-with-shapes"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <p>
   When loading model weights, we needed to instantiate the model class first, because the class
defines the structure of a network. We might want to save the structure of this class together with
the model, in which case we can pass
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     model
    </span>
   </code>
   (and not
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     model.state_dict()
    </span>
   </code>
   ) to the saving function:
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"" title=""torch.save""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">save</span></a><span class=""p"">(</span><span class=""n"">model</span><span class=""p"">,</span> <span class=""s1"">'model.pth'</span><span class=""p"">)</span>
</pre>
   </div>
  </div>
  <p>
   We can then load the model as demonstrated below.
  </p>
  <p>
   As described in
   <a class=""reference external"" href=""https://pytorch.org/docs/main/notes/serialization.html#saving-and-loading-torch-nn-modules"">
    Saving and loading torch.nn.Modules
   </a>
   ,
saving
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     state_dict
    </span>
   </code>
   is considered the best practice. However,
below we use
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     weights_only=False
    </span>
   </code>
   because this involves loading the
model, which is a legacy use case for
   <code class=""docutils literal notranslate"">
    <span class=""pre"">
     torch.save
    </span>
   </code>
   .
  </p>
  <div class=""highlight-default notranslate"">
   <div class=""highlight"">
    <pre><span></span><span class=""n"">model</span> <span class=""o"">=</span> <a class=""sphx-glr-backref-module-torch sphx-glr-backref-type-py-function"" href=""https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"" title=""torch.load""><span class=""n"">torch</span><span class=""o"">.</span><span class=""n"">load</span></a><span class=""p"">(</span><span class=""s1"">'model.pth'</span><span class=""p"">,</span> <span class=""n"">weights_only</span><span class=""o"">=</span><span class=""kc"">False</span><span class=""p"">),</span>
</pre>
   </div>
  </div>
  <div class=""admonition note"">
   <p class=""admonition-title"">
    Note
   </p>
   <p>
    This approach uses Python
    <a class=""reference external"" href=""https://docs.python.org/3/library/pickle.html"">
     pickle
    </a>
    module when serializing the model, thus it relies on the actual class definition to be available when loading the model.
   </p>
  </div>
 </div>
 <div class=""section"" id=""related-tutorials"">
  <h2>
   Related Tutorials
   <a class=""headerlink"" href=""#related-tutorials"" title=""Permalink to this heading"">
    ¶
   </a>
  </h2>
  <ul class=""simple"">
   <li>
    <p>
     <a class=""reference external"" href=""https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html"">
      Saving and Loading a General Checkpoint in PyTorch
     </a>
    </p>
   </li>
   <li>
    <p>
     <a class=""reference external"" href=""https://pytorch.org/tutorials/recipes/recipes/module_load_state_dict_tips.html?highlight=loading%20nn%20module%20from%20checkpoint"">
      Tips for loading an nn.Module from a checkpoint
     </a>
    </p>
   </li>
  </ul>
  <p class=""sphx-glr-timing"">
   <strong>
    Total running time of the script:
   </strong>
   ( 0 minutes  8.163 seconds)
  </p>
  <div class=""sphx-glr-footer sphx-glr-footer-example docutils container"" id=""sphx-glr-download-beginner-basics-saveloadrun-tutorial-py"">
   <div class=""sphx-glr-download sphx-glr-download-python docutils container"">
    <p>
     <a class=""reference download internal"" download="""" href=""../../_downloads/3648b0dccaebca71b234070fe2124770/saveloadrun_tutorial.py"">
      <code class=""xref download docutils literal notranslate"">
       <span class=""pre"">
        Download
       </span>
       <span class=""pre"">
        Python
       </span>
       <span class=""pre"">
        source
       </span>
       <span class=""pre"">
        code:
       </span>
       <span class=""pre"">
        saveloadrun_tutorial.py
       </span>
      </code>
     </a>
    </p>
   </div>
   <div class=""sphx-glr-download sphx-glr-download-jupyter docutils container"">
    <p>
     <a class=""reference download internal"" download="""" href=""../../_downloads/11f1adacb7d237f2041ce267ac38abb6/saveloadrun_tutorial.ipynb"">
      <code class=""xref download docutils literal notranslate"">
       <span class=""pre"">
        Download
       </span>
       <span class=""pre"">
        Jupyter
       </span>
       <span class=""pre"">
        notebook:
       </span>
       <span class=""pre"">
        saveloadrun_tutorial.ipynb
       </span>
      </code>
     </a>
    </p>
   </div>
  </div>
  <p class=""sphx-glr-signature"">
   <a class=""reference external"" href=""https://sphinx-gallery.github.io"">
    Gallery generated by Sphinx-Gallery
   </a>
  </p>
 </div>
</div>
"
